\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%%

\def\useItalian{0}  % 1 = Italian, 0 = English

\def\courseName{Network Algorithms}

\def\coursePrerequisites{
    \begin{itemize}
        \item Progettazione di Algorithmi
    \end{itemize}
}

\def\book{TODO}

% \def\authorName{Simone Bianco}
% \def\email{bianco.simone@outlook.it}
% \def\github{https://github.com/Exyss/university-notes}
% \def\linkedin{https://www.linkedin.com/in/simone-bianco}

\def\authorName{Alessio Bandiera}
\def\email{alessio.bandiera02@gmail.com}
\def\github{https://github.com/aflaag-notes}
\def\linkedin{https://www.linkedin.com/in/alessio-bandiera-a53767223}

% Do not change

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../packages/Nyx/nyx-packages}
\usepackage{../../packages/Nyx/nyx-styles}
\usepackage{../../packages/Nyx/nyx-frames}
\usepackage{../../packages/Nyx/nyx-macros}
\usepackage{../../packages/Nyx/nyx-title}
\usepackage{../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../packages/Nyx/logo.png}

\ifx\useItalian0
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} Università di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \subtitle{Appunti integrati con il libro \book}
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \subtitle{Lecture notes integrated with the book \book}
    \author{\textit{Author}\\\authorName}
\fi

\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

\addbibresource{./references.bib}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%

    \chapter{The routing problem}

    \section{Introduction on graphs}

    In many network applications, graphs are used as a natural model. In other applications, the graph model may be less obvious, but appears to be still very useful. Graph algorithms are useful instruments to solve important and living problems. We will see a number of advanced techniques for efficient algorithm design to solve problems from networks and graphs. 

    \begin{frameddefn}{Graph}
        A \tbf{graph} is a mathematical structure $G = (V,E)$ made of a set $V$ called the \textit{vertex set} (or \textit{node set}), and a set $E \subseteq V \times V$ called \textit{edge set}.
    \end{frameddefn}

    Graphs are usually represented through circles and lines, were each line between two vertices $u,v$ represents the edge $(u,v)$. We will assume to be working with \textit{simple graphs}, a type of graph that doesn't allow loop edges, i.e. edges from a node to itself, or a multiple number of edges between two vertices.  
   

    The edges of a graph can also be \textit{directed} or \textit{undirected}. In the former, the two edges $(u,v)$ and $(v,u)$ are considered two distinct edges while in the latter they are considered as the same edge. A directed graph is usually also referred to as \tbf{digraph}. 
    
    \begin{figure}[H]
        \centering

        \begin{tabular}{ccc}
            \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (6) [right of=2] {6};
                \node[main node] (5) [above right of=6] {5};
                \node[main node] (4) [below of=5] {4};
    
                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    (5) edge (1)
                    (6) edge (3)
                    (6) edge (5)
                    ;
            \end{tikzpicture}

            &\qquad\qquad&

            \begin{tikzpicture}[->,>=stealth,shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [below left of=1] {2};
                \node[main node] (3) [below right of=2] {3};
                \node[main node] (6) [right of=2] {6};
                \node[main node] (5) [above right of=6] {5};
                \node[main node] (4) [below of=5] {4};
    
                \path[every node/.style={font=\sffamily\small}]
                    (1) edge (2)
                    (2) edge (3)
                    (2) edge (4)
                    (3) edge (4)
                    (4) edge (1)
                    (5) edge (1)
                    (6) edge (3)
                    (6) edge (5)
                    ;
            \end{tikzpicture}
        \end{tabular}

        \caption{On the left: a simple graph. On the right: a simple digraph.}
    \end{figure}

    Graphs were born in 1736, when Euler used them to formalize and solve the famous \textit{Seven Bridges of Königsberg} problem: is there a way to walk through all the bridges of the town and end up on the starting point? 

    \begin{figure}[H]
        \centering

        \includegraphics[scale=0.6]{../assets/Konigsberg_bridges.png}
        \caption{The city of Königsberg and its seven bridges.}
    \end{figure}

    To solve the problem, Euler represented the problem as the following \textit{multi-graph}, i.e. a non-simple graph that allows multiple edges between two vertices. Euler proved that the answer to the question is negative: a walk that passes through all the edges of such graph while also returning to the starting node \tbf{cannot exist}.

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
            \node[main node] (1) {};
            \node[main node] (2) [below left of=1, xshift = -50] {};
            \node[main node] (3) [below right of=2, xshift = 50] {};
            \node[main node] (4) [right of=2, xshift = 75] {};

            \path[every node/.style={font=\sffamily\small}]
                (1) edge [out=180, in=90] (2)
                (1) edge [out=-110, in=30] (2)
                (2) edge [out=270, in=180] (3)
                (2) edge [out=-30, in=110] (3)
                (2) edge (4)
                (1) edge (4)
                (3) edge (4)
                ;
        \end{tikzpicture}

        \caption{The multi-graph representing the \tit{Seven Bridges of Königsberg} problem.}
    \end{figure}

    In general, a \tbf{walk} on a graph $G$ is given by a sequence of nodes $v_1, \ldots, v_k$ such that $(v_i, v_{i+1}) \in E(G)$. A \tbf{path} is walk whose vertices are all distinct. As we'll see in the following sections, walks and paths are the basis of graph theory.

    \newpage

    \section{The least cost path problem}

    When packets are sent from a computer to another through a network, each computer has to route data on a path passing through intermediate computers. This problem is usually referred to as the \tbf{routing problem}.
    
    By modelling the network as a graph whose vertices correspond to the computers and its edges correspond to the links between them, such problem is reduced to the concept of a path from an initial node to an arrival node.

    Based on the required conditions, the routing reduces to a specific type of path problem:
    \begin{enumerate}
        \item In \tbf{non-adaptive routing}, the routing algorithm must minimize the number of intermediate computers on the route. This problem reduces to the \textit{shortest path problem}, i.e. finding the path that passes through the lowest amount of edges from node $s$ to node $t$. This type of routing gives good results with consistent topology and traffic conditions, but performs poorly in case of congestion.  Usually this type of routing is implemented through one global \tit{routing table}.
        \item In \tbf{adaptive routing}, the routing algorithm must take into account the traffic conditions: if a route is congested, we want to avoid passing through it. This problem reduces to the \textit{least cost path problem}, i.e. finding the path with the least cost from node $d$ to node $t$. This type of routing gives good results with high network workload, but routes must be computed frequently in order to perform well. In this type of routing, each router creates its own \tit{routing table}.
        \item In \tbf{half-adaptive routing}, depending on traffic and workload on the network, the routing type can switch between \tit{adaptive} and \tit{non-adaptive}.
        \item In \tbf{fault-sensitive routing}, the routing algorithm must consider the possibility of a link failing: we want the route with the highest probability of working. 
    \end{enumerate}

    Each of these problems can be modeled as a graph. In particular, adaptive routing and fault-sensitive routing need an additional \textit{weight function} $w : E(G) \to \R$ such that $w(e)$ represents the weight of an edge $e \in E(G)$. The \tbf{weight (or cost) of a path} $P$, written as $w(P)$, is the sum of its edges.

    \begin{example}[Weighted graphs]
        The following is an example of a graph with weights on the edges.

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=3cm,thick,main node/.style={scale=0.6,circle,draw,font=\sffamily\normalsize}]
                \node[main node] (1) {1};
                \node[main node] (2) [above of=1] {2};
                \node[main node] (3) [right of=2] {3};
                \node[main node] (4) [below of=3] {4};

                \path[every node/.style={font=\sffamily\small}]
                    (1) edge node {1} (2)
                    (2) edge node {2} (3)
                    (3) edge node {5} (4)
                    (4) edge node {10} (1)
                    (1) edge node {6} (3)
                    ;
            \end{tikzpicture}
            \caption{An undirected weighted graph.}
        \end{figure}

        For instance, the path $1, 2, 3, 4$ has weight $1 + 2 + 5 = 8$.
    \end{example}
    
    The weight measure varies based on the context. In adaptive routing the traffic acts as the weight, while in fault-sensitive routing the probability acts as the weight. In particular, let $p(u,v)$ be the probability that an edge $(u,v) \in E(G)$ doesn't fail. Under the not-so-realistic assumption that edge failures occur independently of each other, we get that the probability that a path $P = v_1, \ldots, v_k$ doesn't fail is given by $p(v_1, v_2) \cdot \ldots \cdot p(v_{k-1}, v_k)$.
    
    By setting each weight $w(u,v)$ equal to $-\log(p(u,v))$, we get that the product $p(v_1, v_2) \cdot \ldots p(v_{k-1}, v_k)$ reaches its maximum when the sum $w(v_1, v_2) + \ldots + w(v_{k-1}, v_k)$ reaches its minimum. Through this weight function, fault-sensitive routing is also reduces to the least cost path problem.

    Similarly, the shortest path problem can also be reduced to the least cost path problem by setting $w(u,v)$ equal to 1 for each edge. One problem to rule them all!

    \begin{frameddefn}{Distance}
        Let $G = (V,E)$ be a graph. Given two nodes $u,v \in V(G)$, the \tbf{distance} between $u$ and $v$, written as $\dist(u,v)$, is the minimum weight of all the paths $u \to v$ of $G$.
    \end{frameddefn}

    On digraphs the concept of distance is non-symmetrical: the distance $\dist(u,v)$ may be different from the distance $\dist(v,u)$. Moreover, when there is no path $u \to v$, we assume that $\dist(u,v) = +\infty$.

    Note that for the \tbf{one-to-all} shortest path problem where each edge has unit weight, the problem can be solved through a simple \tit{Breadth-First-Search} (BFS) algorithm, invented by \textcite{moore}.

    Moreover, all known algorithm for finding the least cost path between any two nodes on a graph are based on \tbf{graph exploration}, which is based on multiple \tit{walks} (i.e. paths where vertices may be repeated) on the graph. This raises a problem when there are \tbf{negative-weight cycles}, because the walk could take such a cycle infinitely many times, and the weight of the path between the two nodes can be lowered infinitely, without halting. Therefore, only networks \tit{without} negative-weight cycles will be discussed.

    Therefore, in any solution of the least cost path problem:

    \begin{itemize}
        \item cycles having \tbf{negative weight} cannot exist, by hypothesis;
        \item cycles having \tbf{positive weight} cannot exist, by contradiction: if there is such a cycle in a solution, then a solution without the cycle would yield a lower-weight path;
        \item cycles having \tbf{null length} cannot exist, by assumption: if there is such a cycle in a solution, then a solution without the cycle would yield a path of the same weight;
    \end{itemize}

    Hence, we can assume that there exists at least one solution \tbf{without any cycle}.
    
    \subsection{Classical algorithms}
    
    All the classical algorithms that will be described are based on the \tbf{relaxation} principle. Given a graph $G$, a weight function $w$, and a starting vertex $s \in V(G)$, let $\func{d}{V(G)}{\R}$ be a function that represents the current \tit{estimate} of the distance from $s$ to any other node. At the beginning, $d(v) := + \infty$ for each $v \in V(G)$. Then, a \tbf{relaxation step} is performed as follows: given an edge $(u, v) \in E(G)$, if $d(u) + w(u, v) < d(v)$, then we set $d(v) = d(u) + w(u, v)$.

    The first papers that presented a solution to the least cost path problem were published by \textcite{bellman} and \textcite{ford} independently, which described the following algorithm.

    \begin{framedalgo}{Bellman-Ford}
        Given a graph $G$, a weight function $\func{w}{E(G)}{\R}$ on the edges, and an input node $s$, the algorithm returns the minimum distance tree rooted in $s$ as a parent array, based on $w$. \\
        \hrule

        \quad
        \label{alg:bellman_ford}
        \begin{algorithmic}[1]
            \Function{bellmanFord}{$G$, $w$, $s$}
                \For{$v \in V(G)$}
                    \State $d(v) := +\infty$
                \EndFor
                \State $\texttt{p} := \texttt{[}\texttt{NULL}, \ldots, \texttt{NULL}\texttt{]}$
                \For{$i \in [1, n - 1]$}
                    \For{$(u, v) \in E(G)$}
                        \If{$d(u) + w(u, v) < d(v)$} \Comment{relaxation step}
                            \State $d(v) = d(u) + w(u, v)$
                            \State $\texttt{p[}v\texttt{]} = u$
                        \EndIf
                    \EndFor
                \EndFor
                \State \textbf{return} \ttt{p}
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        The algorithm updates each distance $\dist(s, v)$ for any $v \in V(G)$ progressively: for instance, in the first iteration of the \texttt{for} loop in line 6, since each distance is set to $+\infty$, only $s$'s neighbours will be updated. This will be repeated by \curlyquotes{expanding} the updated vertices progressively at each iteration, exactly $n - 1$ times, because a path has at most $n - 1$ nodes since we are assuming that our solution does not contain cycles.
    }

    \cost{
        The cost of the algorithm is simply given by $$O(n) + O((n - 1) \cdot m) = O(nm)$$
    }

    The Bellman-Ford algorithm is used for the \href{https://en.wikipedia.org/wiki/Distance-vector_routing_protocol}{distance vector routing protocol}, an iterative, asynchronous and distributed protocol.

    One year later, \textcite{dijkstra} presented the following algorithm, which lowered the computational cost of the Bellman-Ford algorithm. In fact, each step of the latter iterates on all the nodes in $G$, even when the majority will not be updated.

    Note that the following algorithm lowers the time complexity, at the cost of reducing the generality of the algorithm, because the weight function $w$ can only be defined on $\R^+$.

    \begin{framedalgo}{Dijkstra}
        Given a graph $G$, a weight function $\func{w}{E(G)}{\R^+}$ on the edges, and an input node $s$, the algorithm returns the minimum distance tree rooted in $s$ as a parent array, based on $w$. \\
        \hrule

        \quad
        \label{alg:dijkstra}
        \begin{algorithmic}[1]
            \Function{dijkstra}{$G$, $w$, $s$}
                \For{$v \in V(G)$}
                    \State $d(v) := +\infty$
                \EndFor
                \State $\texttt{p} := \texttt{[}\texttt{NULL}, \ldots, \texttt{NULL}\texttt{]}$
                \State $S := \varnothing$
                \State $Q := V(G)$ \Comment{$Q$ is based on $d$}
                \While{$Q \neq \varnothing$}
                    \State $u := Q\ttt{.extract\_min()}$
                    \State $S = S \cup \{u\}$
                    \For{$(u, v) \in E(G)$}
                        \If{$d(u) + w(u, v) < d(v)$} \Comment{relaxation step}
                            \State $d(v) = d(u) + w(u, v)$
                            \State $\texttt{p[}v\texttt{]} = u$
                            \State $Q\texttt{.update()}$ \Comment{updating $v$'s value in $Q$}
                        \EndIf
                    \EndFor
                \EndWhile
                \State \textbf{return} \ttt{p}
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        The algorithm expands $S$, i.e. the set of visited nodes, iteratively, and at each iteration:

        \begin{itemize}
            \item the closest node $u$ to $s$ is choosen, based on $d(u)$;
            \item for each outgoing edge $(u, v)$ from $u$, $v$ is relaxed w.r.t $(u, v)$, and $Q$ is updated based on $d(v)$.
        \end{itemize}
    }

    \cost{
        The cost of the algorithm depends on the implementation:

        \begin{itemize}
            \item if $Q$ is implemented through a \tit{queue}, then the time complexity is $O(n^2)$
            \item if $Q$ is implemented through a \tit{heap}, then the time complexity is $O(m \log n)$
            \item if $Q$ is implemented through a \tit{fibonacci heap}, then the time complexity is $O(m + n \log n)$
        \end{itemize}
    }

    The last algorithm that will be discussed was discovered independetly by \textcite{floyd} and \textcite{warshall}, which solves the \tbf{all-to-all} version of the least cost path problem.

    \begin{framedalgo}{Floyd-Warshall}
        Given a directed graph $G$, and an unconstrained weight function $w$ for the edges, the algorithm returns a matrix \texttt{dist} such that $\arraytt{dist}{u}{v}$ is the weight of the least-cost path from $u$ to $v$. \\

        \hrule

        \quad
        \label{alg:floyd_warshall}
        \begin{algorithmic}[1]
            \Function{floydWarshall}{$G$, $w$}
                \State Let $\texttt{dist[}n\texttt{][}n\texttt{]}$ be an $n \times n$ matrix, initialized with every cell at $+ \infty$
                \For{$u \in V(G)$}
                    \State $\arraytt{dist}{u}{u} = 0$
                \EndFor
                \For{$(u, v) \in E(G)$}
                    \State $\arraytt{dist}{u}{v} = w(u, v)$
                \EndFor
                \For{$k \in V(G)$}
                    \For{$u \in V(G)$}
                        \For{$v \in V(G)$}
                            \State $\arraytt{dist}{u}{v} = \min \rbk{\arraytt{dist}{u}{v}, \arraytt{dist}{u}{k} + \arraytt{dist}{k}{v}}$
                        \EndFor
                    \EndFor
                \EndFor
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}
   
    \idea{
        The core concept of the algorithm is to construct a matrix using a \href{https://en.wikipedia.org/wiki/Dynamic_programming}{dynamic programming} approach, that evaluates all possible paths between every pair of vertices. Specifically, to determine the shortest path from a vertex $u$ to a vertex $v$, the algorithm considers two options: either traveling directly from $u$ to $v$, or passing through an intermediate vertex $k$, potentially improving the path.
    }

    \cost{
        The \texttt{for} loop in line 3 has cost $\Theta(n)$, the \texttt{for} loop in line 6 has cost $\Theta(m) = \Theta(n^2)$ and the cost of the triple nested \texttt{for} loop is simply $\Theta(n^3)$. Therefore, the cost of the algorithm is $$\Theta(n) + \Theta(n^2) + \Theta(n^3) = \Theta(n^3)$$
    }

    \section{Interconnection topologies}

    Up to this point, the routing problem has considered the network as a graph where \tbf{the structure is not known to the nodes}, and can change over time due to factors like \tit{faults} and \tit{variable traffic}. However, when the network represents an \tbf{interconnection topology}, such as the one connecting processors, the structure of the network is known and remains fixed. This characteristic can be leveraged in the packet-routing algorithms.

    While the fixed nature of the network topology can be used to develop more efficient routing strategies, efficiency becomes a critical concern in interconnection topologies. As a result, solutions with stronger properties than basic shortest-path algorithms are required.

    There are many types of routing models. In this notes, the focus will be on the \href{https://en.wikipedia.org/wiki/Store_and_forward}{store-and-forward} model:

    \begin{itemize}
        \item data is divided into \tit{discrete packets};
        \item each packet contains \tit{control information} (such as source, destination, and sequence data) and is treated as an independent unit that is forwarded from node to node through the network;
        \item packets may be temporarily stored in \tbf{buffer queues} at intermediate nodes if necessary, due to link congestion or busy channels;
        \item each node makes a \tbf{local routing decision} based on the packet's destination address and the chosen routing algorithm;
        \item during each step of the routing process, \tbf{a single packet can cross each edge};
        \item additionally, mechanisms for error detection and recovery may be employed to ensure reliable packet delivery, and flow control and congestion management may be applied to optimize network performance.
    \end{itemize}

    \subsection{Butterfly networks} \label{butterfly_networks}

    \begin{frameddefn}{Butterfly network}
        Let $n$ be an integer, and let $N := 2^n$; an \tbf{$n$-bufferfly network} is a \tit{layered graph} defined as follows:

        \begin{itemize}
            \item there are $n + 1$ layers of $N$ nodes each, for a total of $N(n + 1)$ nodes;
            \item each node is labeled with a pair $(w, i)$, where $i$ is the \tit{layer of the node}, and $w$ is an $n$-bit binary number that denotes the \tit{row of the node};
            \item there are $2Nn = 2 \cdot 2^n \cdot n = n2^{n + 1}$ edges;
            \item two nodes $(w, i)$ and $(w', i')$ are linked by an edge if and only if $i' = i + 1$ and either $w = w'$ (which is a \tit{straight edge}) or $w$ and $w'$ differ in only the $i$-th bit (which is a \tit{cross edge}).
        \end{itemize}
    \end{frameddefn}

    \begin{example}[Butterfly network]
        The following figure shows an example of a butterfly network.

        \centeredimage[A butterfly network.]{0.35}{../assets/butterfly.png}
    \end{example}

    Note that the nodes of a butterfly network can be \tbf{rearranged} to form a mirror image of the original network.

    Butterfly networks have a \tbf{recursive structure}, which is highlighted in the following figure. Specifically, one $n$-dimensional butterfly contains two $(n - 1)$-dimensional butterfly networks as subgraphs.

    \centeredimage[The recursive structure of butterfly networks.]{0.3}{../assets/butterfly_recursive.png}

    Through the recursive structure of the butterfly network it can be easily shown, by structural induction, that each node of the network has degree 4, except for the ones in the first and last layer. Therefore, to perform the routing of the packets on a butterfly network, its nodes are made of \tbf{crossbar switches}, which have two input and two output ports and can operate in two states, namely \tit{cross} and \tit{bar} (shown below, respectively).

    \centeredimage[A butterfly network node.]{0.25}{../assets/butterfly_nodes.png}

    Usually, $4N$ additional nodes are typically added ($2N$ for the input, and $2N$ for the output) such that $\deg(u) = 4$ for each $u \in V(G)$ --- these nodes will not be considered in the networks analyzed in this notes.

    \centeredimage[An extended butterfly network.]{0.25}{../assets/butterfly_extended.png}

    As a result, a butterfly network can be viewed as a \tit{switching network} that connects $2N$ input units to $2N$ ouptut units, through a layered structure divided into $\log N +1 = \log 2^n +1 = n + 1$ layers, each consisting of $N$ nodes.

    The topology of the butterfly network can be leveraged as stated in the following proposition.

    \begin{framedprop}[label={prop:greedy_path}]{Greedy path}
        Given a pair of rows $w$ and $w'$, there exists a \tit{unique path of length $n$}, called \tbf{greedy path}, from node $(w, 0)$ to node $(w', n)$. This path passes through each layer exactly once, and it can be found through the following procedure:

        \begin{algorithmic}[1]
            \Function{greedyPath}{$w$, $w'$}
                \For{$i \in [1, n]$}
                    \If{$w_i == w'_i$}
                        \State Traverse a \tit{straight edge}
                    \Else
                        \State Traverse a \tit{cross edge}
                    \EndIf
                \EndFor
            \EndFunction
        \end{algorithmic}
    \end{framedprop}

    Packet-routing performed on a butterfly network can pose some challenges. Assume that each node $(u, 0)$ in the network on layer 0 of the butterfly contains a packet, which is destined for node $(\pi(u), n)$ in layer $n$ --- there are $n + 1$ layers, ranging in $[0, n]$ --- where $$\func{\pi}{[1, N]}{[1, N]}$$ describes the permutation of the packet destinations. In a \tbf{greedy routing algorithm}, each packet follows its \tit{greedy path}, meaning that at each intermediate layer, it makes progress toward its final destination by choosing the edges to cross through the algorithm described in \cref{prop:greedy_path}.

    When routing only a \tit{single packet}, the greedy algorithm works efficiently, since there are no conflicts or competing resources along the path. However, when \tit{multiple packets} are routed in parallel, conflicts can arise, especially when multiple packets attempt to traverse the same edge or node simultaneously. In fact, \tit{multiple greedy paths} may intersect at the same node or edge, and since only one packet can traverse a given edge at any moment, the other packets must be \tbf{delayed} until the edge becomes available. As a result, the butterfly network cannot route every permutation without delays, making it a \tbf{blocking network}.

    For simplicity, assume that $n$ is odd (though similar results hold for even values of $n$), and consider the following edge $$e :=  \rbk{ \rbk{0 \ldots 0, \frac{n-1}{2}},  \rbk{ 0 \ldots 0,\frac{n+1}{2} } }$$ Note that $e$'s endpoints are the roots of two complete binary trees, which have $2^{\frac{n -1}{2}}$ and $2^{\frac{n + 1}{2}}$ nodes respectively.

    \centeredimage{0.3}{../assets/butterfly_trees.png}

    In the worst case, $\pi$ can be such that \tit{each greedy path starting from a leaf on the left tree and ending on a leaf on the right tree traverses $e$}. Note that the number of such paths is precisely the number of leaves of the left complete binary tree, namely $2^\frac{n- 1}{2} = \sqrt{\frac{N}{2}}$. Therefore, in the worst case $\sqrt{\frac{N}{2}}$ packets may need to traverse $e$, which means that one of them may be delayed by $\sqrt{\frac{N}{2}} - 1$ steps. Since it takes $n = \log N$ steps to traverse the whole network, the greedy algorithm can take up to $$\sqrt{\dfrac{N}{2}} -1 + \log N$$ steps to route a permutation.

    The following theorem generalizes this result.

    \begin{framedthm}{Butterfly routing}
        Given any routing problem on a $n$-dimensional butterfly network, for which at most one packet starts at each $0$-th layer node, and at most one packet is destined for each $n$-th layer node, the \tit{greedy algorithm} will route all the packets to their destination in $O (\sqrt N)$ steps.
    \end{framedthm}

    \begin{proof}
        For simplicity, assume that $n$ is odd (though similar results can be proven for even values of $n$). Given $0 < i \le n$, let $e$ be any edge in the $i$-th layer, and let $n_i$ be the number of greedy paths traversing $e$.

        The number of greedy paths in the first half of the butterfly is bounded by the number of leaves of the left complete binary tree, namely $n_i \le 2^{i - 1}$. Analogously, on the second half of the butterfly, $n_i$ is bounded by the number of leaves of the right complete binary tree, therefore $n_i \le 2^{n - i}$. Note that both this results hold because $n$ is odd.

        Note that any packet that needs to cross $e$ can be delayed by \tit{at most} the other $n_i - 1$ packets. Therefore, recalling that $\displaystyle \sum_{j = 0}^k {2^j} = 2^{k + 1} - 1$, as a packet traverses layers 1 through $n$, the total delay it can encounter is at most

        \begin{equation*}
            \begin{split}
                \sum_{i = 1}^n {(n_i -1)} &= \sum_{i = 1}^\frac{n + 1}{2} {(n_1 - 1)} + \sum_{i = \frac{n + 1}{2} + 1}^n(n_i - 1) \\
                                          &\le \sum_{i = 1}^\frac{n + 1}{2} {\rbk{2^{i - 1} - 1}} + \sum_{i = \frac{n + 3}{2}}^n {\rbk{2^{n - i} - 1}} \\
                                          &= \sum_{j = 0}^{\frac{n + 1}{2} - 1}{\rbk{2^j - 1}} + \sum_{j = 0}^{\frac{n - 3}{2}} {\rbk{2^j - 1}} \\
                                          &=\sum_{j = 0}^{\frac{n + 1}{2} - 1}{2^j} + \sum_{j = 0}^{\frac{n - 3}{2}} {2^j} - n \\
                                          &=2^{\frac{n + 1}{2}} - 1 + 2^{\frac{n - 1}{2}} - 1 - n \\
                                            &\le O(\sqrt N) - n \\
                                            &\le O(\sqrt N)
            \end{split}
        \end{equation*}
    \end{proof}

    Although such a greedy routing algorithm performs poorly in the worst case, it is \tbf{highly effective in practice}. In fact, for many practical classes of permutations, the greedy algorithm runs in $n$ steps, which is optimal, and for most permutations the algorithm runs in $n + o(n)$ steps. Consequently, the greedy algorithm is widely used in real-world applications.

    \subsection{Beneš networks}

    As shown in the previous section, the \tit{butterfly network} can present efficiency problems due to packets' delays caused by congestion when multiple packets are routed simultaneously. One way to \tit{avoid routing delays} is by using a \tbf{non-blocking topology}.

    \begin{frameddefn}{Beneš network}
        An \tbf{$n$-dimensional Beneš network} is a network constructed by placing \tit{two $n$-dimensional butterfly networks back-to-back}.
    \end{frameddefn}

    \begin{example}[Beneš network]
        The following is an example of a Beneš network.

        \centeredimage[A Beneš network.]{0.2}{../assets/benes.png}
    \end{example}

    Note that an $n$-dimensional Beneš network has $$2(n + 1) - 1 = 2n + 2 - 1 = 2n + 1$$ layers, because the two $n$-dimensional butterfly networks --- which describe the first and last $n + 1$ layers --- have an \tit{overlapping layer}.

    Consider the following property.

    \begin{frameddefn}{Rearrangeability}
        A network with $N$ inputs and $N$ outputs is said to be \tbf{rearrangeable} if, for any one-to-one mapping $\pi$ of the inputs to the outputs, the mapping can be realized using exclusively \tit{edge-disjoint paths}.
    \end{frameddefn}
    
    As for the case of the butterfly network, two inputs and two outputs are typically connected at both the beginning and end of the Beneš network, ensuring that each node has a degree of 4. Therefore, this type of Beneš network has $2N = 2 \cdot 2^n = 2^{n + 1}$ inputs linked to the $0$-th layer, and $2^{n + 1}$ outputs linked to the $2n$-th layer.

    The following theorem will establish an important result that leverages these additional inputs and outputs.

    \begin{framedthm}{Rearrangeability of the Beneš network}
        Any $n$-dimensional Beneš network is rearrangeable.
    \end{framedthm}

    \proofind{
        The proof proceeds by induction on $n$.
    }{
        When $n = 0$, the Beneš consists of a single node, hence the theorem is vacuously true, because there are no edges on the network.
    }{
        Given any one-to-one mapping $\pi$ of the $2^n$ inputs and outputs of a $(n - 1)$-dimensional Beneš network, there exists \tit{a set of edge-disjoint paths} from the inputs to the outputs, connecting each input $i$ to output $\pi(i)$, for each $1 \le i \le 2^{n}$.
    }{
        Consider an $n$-dimensional Beneš network, with $2^{n + 1}$ inputs and outputs; note that its middle $2n - 1$ layers describe two $(n - 1)$-dimensionl Beneš networks, as shown in figure.

        \centeredimage[Subnetworks of a Beneš network.]{0.2}{../assets/benes_subnetworks.png}


        Note that each \tit{starting node} --- those in layer 0 --- has degree 4, and 2 of the links connect each starting node to the inputs, external to the Beneš network. Therefore, by definition of the Beneš network, the remaining two edges must connect each starting node to the two separate $(n - 1)$-dimensional Beneš networks. Formally, each input $2i -1$ and $2i$ must use different Beneš subnetworks, for each $1 \le i \le 2n$.

        The proof is constructive, and involves a so called \tbf{looping algorithm}, which proceeds as follows:

        \begin{itemize}
            \item let two inupts connected to the same starting node be referred to as \tit{mates};
            \item without loss of generality, start by routing input 1 to its destination, defined by $\pi(1)$; note that, as stated previously, this node will traverse only one of the two unconnected $(n - 1)$-dimensional Beneš subnetworks;
            \item route $\pi(1)$'s mate to its input, by traversing the Beneš subnetwork that \tit{was not} traversed by the path $1 \to \pi(1)$;
            \item keep routing back and forth packets through the $n$-dimensional Beneš network; eventually, it will be routed the first input's \tit{mate}, which closes a routing loop;
            \item open another loop and continue routing packets as described.
        \end{itemize}

        Finally, note that routing within the $(n - 1)$-dimensional Beneš networks is assumed to be achievable with edge-disjoint pahts inductively.
    }

    If the Beneš network has \tit{1 single input and output connected to layers 0 and $2n$ respectively}, the following \tit{stronger} theorem can be proven.

    \begin{framedthm}{Node-disjoint paths in Beneš networks}
        Given any one-to-one mapping $\pi$ of the $2^n$ inputs and outputs of an $n$-dimensional Beneš network, there exists \tit{set of node-disjoint paths} from the inputs to the outputs, connecting each input $i$ to output $\pi(i)$, for each $1 \le i \le 2^n$.
    \end{framedthm}

    \begin{proof}
        Details are omitted, because it is analogous to the proof of the previous theorem, but since there is a single input and a single ouptut connected to layer 0 and $2n$ respectively, the \tit{mate} of an input $i$ is input $i + 2^{n - 1}$, for each $1 \le i \le 2^{n - 1}$.

        \centeredimage[Mates in this type of Beneš network.]{0.3}{../assets/benes_single.png}
    \end{proof}

    Although rearrangeability can be achieved, and even node-disjoint paths can be employed to route packets on Beneš networks, both versions of the \tbf{looping algorithm} have notable drawbacks:

    \begin{itemize}
        \item a \tbf{global controller} is \tit{required} to manage the network, determining the routing for each packet, knowing the permutation $\pi$ of the packets;
        \item every time a new permutation $\pi$ needs to be routed, it takes $\Theta(N \log N)$ time to reconfigure all the switches.
    \end{itemize}
    
    \subsection{Mesh networks}

    Another important and widely used interconnection topology is the \tbf{mesh network}, which is described as follows.

    \begin{frameddefn}{Mesh network}
        Given two integers $m, n \ge 1$, an $m \times n$ \tbf{mesh network} $M_{m, n}$ is defines as follows:

        \begin{itemize}
            \item the nodes of the network are labeled by the following cartesian product $$\{1, \ldots, m\} \times \{1, \ldots, n\}$$
            \item there is an edge between nodes $\abk{i,j}$ and $\abk{i',j'}$ if and only if $$\abs{i - i'} + \abs{j - j'} = 1$$
            \item the path comprising the nodes labeled with $\{i\} \times \{1, \ldots n\}$ define the $i$-th row of the network; analogously, the set $\{1, \ldots, m\} \times \{j\}$ define the $j$-th column.
        \end{itemize}
    \end{frameddefn}

    \begin{example}[Mesh network]
        placeholder \todo{add pic}
    \end{example}

    For the convenience of physical layout, mesh networks are the most used topologies in \href{https://en.wikipedia.org/wiki/Network_on_a_chip}{Network-on-Chip} (NoC) design; however, this network will not be explored in these notes.

    \chapter{The interconnection topology layout problem}
    
    The \tbf{interconnection topology layout problem} is a crucial challenge in \href{very-large-scale integration} (VLSI) design, the process of creating an \href{https://en.wikipedia.org/wiki/Integrated_circuit}{integrated circuit} (IC) by combining billions of \href{https://en.wikipedia.org/wiki/MOSFET}{MOS} transistors onto a single chip. It involves finding the most efficient way to place and connect various components (such as transistors, resistors, and other circuit elements) on a silicon chip. The goal is to optimize several factors, including \tit{space}, \tit{power consumption}, \tit{signal delay}, and \tit{manufacturing cost}. This problem becomes particularly important as modern chips contain billions of transistors and require complex interconnections between components.

    The problem originated in the 1940s, during the early stages of digital computing. However, at that time, the technology was not advanced enough to implement complex circuit layouts in an efficient manner. Physical constraints, costs, and the lack of sophisticated computational methods limited the practical application of these ideas.

    In recent decades, as technology advanced, VLSI design has evolved to allow highly dense and intricate circuits in both 2D and 3D layouts. This made the \tbf{interconnection topology layout problem} a crucial area of study, particularly for \tit{optimizing performance}, \tit{reducing power consumption}, and \tit{controlling costs} in increasingly smaller chip designs.

    \section{The orthogonal grid drawing problem}

    To address the challenge of finding efficient ways to place and route the components of a VLSI circuit, while maintaining certain spatial constraints, Clark Duncan Thompson developed the Thompson's Model \cite{thompson}, which involves representing the circuit as a \href{https://en.wikipedia.org/wiki/Graph_drawing}{graph drawing}, and analyzing how the layout corresponds to graph drawing principles.

    \begin{frameddefn}{Graph drawing}
        Given a graph $G$, its \tbf{drawing} $\Gamma$ is a function that

        \begin{itemize}
            \item maps each node $v \in V(G)$ to a distinct point $\Gamma(v)$ in the drawing
            \item maps each edge $(u, v) \in E(G)$ in an open Jordan curve $\Gamma(u, v)$, that starts from $\Gamma(u)$ and ends in $\Gamma(v)$, such that it does not cross any point that is the mapping of a node.
        \end{itemize}
    \end{frameddefn}

    Thompson performed the following mapping, between \tit{VLSI circuits} and \tit{graphs}:

    \begin{itemize}
        \item the \tit{various components} of the VLSI circuit, such as \tit{ports}, \tit{switches} and other electronic elements, are represented by \tbf{nodes} in a graph;
        \item the \tit{wires}, or connections, between the components are represented by \tbf{edges} in a graph.
    \end{itemize}

    However, due to the following spatial constraints imposed by VLSI technology manufacturing, this simple model requires further refinement in order to define a good \tbf{drawing} of a graph.

    \begin{itemize}
        \item \tbf{Orthogonal drawing}: \tit{slanting lines} (diagonal connections) between components can only be \tit{approximated}, using small horizontal and vertical segments, because of the limitations in how the VLSI fabrication process manufactures the connections onto the \href{https://en.wikipedia.org/wiki/Wafer_(electronics)}{silicon wafer}. This forces the drawing to be \tbf{orthogonal}, which means that \tit{edges are represented as broken lines}, whose segments are horizontal or vertical, parallel to the coordinate axes.
            \centeredimage[An orthogonal drawing.]{0.15}{../assets/orthogonal.png}
        \item \tbf{Grid drawing}: maintaining \tit{adequate spacing} between wires is crucial to \tit{prevent interference}, which can degrade signal integrity. Proper spacing reduces parasitic capacitance and inductance, ensuring faster signal transmission and lower power consumption. Therefore, the graph drawing must be a \tbf{grid drawing}, such that all nodes, and crosses and bends of all the edges are put on grid points, on a grid plane, where the \tit{grid unit} is the minimum distance allowed between two wires.
            \centeredimage[A grid drawing.]{0.2}{../assets/grid.png}
        \item \tbf{Crossing number minimization}: wires \tit{must not cross}, to avoid interference and signal integrity issues. To manage this constraint, designers often route wires on opposite sides of the circuit board, utilizing small \curlyquotes{holes} that create vertical connections between layers. While this technique helps prevent crossings, it is essential to \tbf{minimize} the number of such holes, as their fabrication can be \tit{expensive} and may complicate the manufacturing process.
        \item \tbf{Area minimization}: silicon is a \tit{costly material}, making it essential to minimize the layout area of integrated circuits. Compact layouts not only reduce material costs, but also enhance performance by shortening wire lengths, which decreases signal delay and power consumption. Therefore, \tbf{area minimization} is a critical objective in the design process, as efficient use of silicon can lead to functional advantages in the final product.
        \item \tbf{Edge length minimization}: wire lengths must be kept \tit{short}, because propagation delay increases with wire length, negatively impacting circuit performance. In layered topologies, it is particularly important that wires within the same layer are approximately equal in length to \tit{prevent synchronization issues} between signals. Thus, \tbf{edge length minimization} is crucial, as it helps ensure faster signal transmission and consistent timing across the circuit.
    \end{itemize}

    In 1980, Thompson introduced the following model, which describes how to draw the graph of a circuit to comply with the aforementioned constraints of VLSI design.

    \begin{frameddefn}{Thompson's Model}
        Given a graph of a topology $G$, the \tbf{Thompson's Model} defines its layout drawing as a \tit{plane representation}, composed of a multitude of \tit{unit-distance horizontal and vertical traces}. This layout adheres to the following criteria:

        \begin{itemize}
            \item every \tit{node} in $V(G)$ is mapped to the \tit{intersection points} of the traces;
            \item every \tit{edge} in $E(G)$ is represented by \tit{disjoint paths}, formed by horizontal and vertical segments along the traces; these paths \tit{must not} intersect nodes that are not their endpoints, and they can only cross each other at designated trace intersection points.
            \item \tit{overlappings}, \tit{node-edge crosses} and \tit{\curlyquotes{knock-knees}} are not allowed;
                \centeredimage[An overlapping, a node-edge cross, and a \tit{knock-knee}.]{0.25}{../assets/not_allowed.png}
        \end{itemize}
    \end{frameddefn}

    In other words, this definition states that the layout of the graph of a circuit should be drawn through an \tbf{orthogonal grid drawing}, which is defined as follows.

    \begin{frameddefn}{Orthogonal grid drawing}
        An \tbf{orthogonal grid drawing} of a given graph $G$ is a bijection, such that:

        \begin{itemize}
            \item each node $v \in V(G)$ is mapped to \tit{plane points} $\Gamma(v)$ at \tit{integer coordinates};
            \item each edge $(u, v) \in E(G)$ is mapped to \tit{non-overlapping paths}, such that the images of the endpoints $\Gamma(u)$ and $\Gamma(v)$ are connected by the corresponding paths;
            \item each path is constituted by \tit{horizontal and vertical segments}, and each possible bend lies on \tit{integer coordinates}.
        \end{itemize}
    \end{frameddefn}

    \begin{framedobs}{Orthogonal grid drawings}
        Note that only graphs with $\deg(v) \le 4$ for each $v \in V(G)$ can be correctly drawn.
    \end{framedobs}

    Hence, the \tbf{interconnection topology layout} is an \tbf{orthogonal grid drawing} of the corresopnding graph, aimed at \tit{minimize} the \tit{area}, the \tit{number of crossings} and the \tit{wire length}.

    The literature on graph drawing is extensive, but it is \tit{not possible} to apply \tbf{existing algorithms} for orthogonal grid drawing to address the layout problem. In fact, while these algorithms provide \tit{certain bounds} on optimization functions, for any input graph meeting specified criteria, interconnection topologies are typically \tbf{highly structured graphs}, often regular, symmetric, or recursively built. By leveraging these unique properties, it is possible to achieve \tit{significantly better results}. General graph drawing algorithms take a graph as input and create a planar representation; in contrast, \tbf{layout algorithms} are \tit{specifically designed} for \tit{particular interconnection topologies}, and require only the dimensions of the topology as input. This implies that each interconnection topology will necessitate its \tbf{own tailored algorithm}.

    It's also noteworthy that improving an optimization function by even a \tit{constant} factor can have \tbf{substantial implications}, particularly concerning area optimization. For example, if one layout occupies half the area of another, it effectively \tit{reduces costs by half}, making such optimizations critically important.

    The following sections will explore some interconnection topologies and their own orthogonal grid drawing algorithms.

    \subsection{H trees}

    An efficient algorithm for generating an orthogonal grid drawing of a \tbf{$n$-node complete binary tree} has been found independently by \textcite{leiserson} and \textcite{valiant}, which employs \href{https://en.wikipedia.org/wiki/H_tree}{H trees}, which are defined as follows.

    \begin{frameddefn}{H tree}
        An \tbf{H tree} organizes a complete binary tree such that \tit{only horizontal and vertical lines} connect the nodes. It can be defined inductively from its height $h$ as follows:
        \begin{itemize}
            \item if $h = 0$ then a single node is sufficient
                \centeredimage[An H tree of height $h = 0$.]{0.1}{../assets/0_h_tree.png}
            \item otherwise, given two H trees of height $h - 1$, connect them as shown in the left drawing if $h$ is even, otherwise use the rightmost construction if $h$ is odd.
                \centeredimage[The inductive step of the inductive H tree construction.]{0.2}{../assets/h_tree_induct.png}
        \end{itemize}
    \end{frameddefn}

    \begin{example}[H trees]
        The following figure shows an example of an H tree of height $h = 4$.

        \centeredimage[H tree of height $h = 4$.]{0.2}{../assets/h_tree_ex.png}
    \end{example}

    \textcite{leiserson} and \textcite{valiant} showed that an H tree can be represented in an area of $O(n)$, where $n$ is the number of nodes of the H tree --- trivially, the area must be $\Omega(n)$. However, $O(n)$ is not sufficient, and the constant factor concealed by the big $O$ notation must also be considered. Additionally, \textcite{brent} proved that, if the leaves of a binary tree are required to be positioned along the borders of the rectangular area, the layout must occupy $\Omega (n \log n)$ area instead.

    Note that the area of the grid we are considering is the following.

    \centeredimage[The grid of the H tree.]{0.15}{../assets/symphony.png}

    \begin{framedthm}{Area of an H tree}
        The area occupied by an $n$-node H tree is $2(n + 1) + o(n)$.
    \end{framedthm}

    \proofind{
        The proof proceeds by induction on the height of $h$ the H tree
    }{
        There are 3 base cases, namely when $h = 0$, $h = 1$ and $h = 2$, respectively shown in the figure below.
        \begin{figure}[H]
            \centering
            \begin{tabular}{ccccc}
                \begin{tabular}{c}\includegraphics[scale=0.1]{../assets/0_h_tree.png}\end{tabular} & \quad & \begin{tabular}{c}\includegraphics[scale=0.15]{../assets/h_tree_3.png}\end{tabular} & \quad & \begin{tabular}{c}\includegraphics[scale=0.15]{../assets/h_tree_7.png}\end{tabular}
            \end{tabular}
            \caption{Cases for $h = 0$, $h = 1$ and $h = 2$.}
        \end{figure}
        Let $l_h$ and $w_h$ be the two sides of the rectangle enclosing the H tree of height $h$, respectively; thus, we have that
        
        \begin{itemize}
            \item for $h = 0$, $l_0 = w_0 = 2 \implies A_0 = l_0 \cdot w_0 = 2 \cdot 2 = 4 = 2(1+1)$ 
            \item for $h = 1$, $l_1 = 2$ and $w_1 = 4$, therefore $$A_1 = l_1 \cdot w_1 = 2 \cdot 4 = 8 = 2(3+1)$$
            \item for $h = 2$, $l_2 = w_2 = 4 \implies A_2 = l_2 \cdot w_2 = 4 \cdot 4 = 16 = 2(7+1)$
        \end{itemize}
    }{
        Assume the result is true for an H tree of height $h - 1$.
    }{
        Two different cases must be analyzed, specifically when $h$ is \tit{odd} an $h$ is \tit{even}.
        
        \begin{itemize}
            \item For the \tit{odd} case, the sides of the rectangle are defined as follows: $$\soe{l}{l_h = l_{h - 1} = 2l_{h - 2} \\ w_h = 2w_{h - 1} = 2w_{h - 2}}$$ (note that $l_{h - 1} = 2l_{h - 2}$ and $w_{h - 1} = w_{h - 2}$). Therefore
                \begin{equation*}
                    \begin{split}
                        l_h &= 2l_{h-2} \\
                            &= \ldots \\
                            &= 2^k \cdot l_{h - 2k} \quad \quad \rbk{h - 2k = 1 \implies k = \frac{h - 1}{2}} \\
                            &= 2^\frac{h - 1}{2} \cdot l_1 \\
                            &= 2^\frac{h - 1}{2} \cdot 2 \\
                            &= 2^{\frac{h - 1}{2} + 1} \\
                            &= 2^\frac{h + 1}{2}
                    \end{split}
                \end{equation*}
                and analogously
                \begin{equation*}
                    \begin{split}
                        w_h &= 2w_{h - 2} \\
                            &= \ldots \\
                            &= 2^k \cdot w_{h - 2k} \quad \quad \rbk{h - 2k = 1 \implies k = \frac{h - 1}{2}} \\
                            &= 2^\frac{h - 1}{2} \cdot w_1 \\
                            &= 2^\frac{h - 1}{2} \cdot 4 \\
                            &= 2^{\frac{h - 1}{2} + 2} \\
                            &= 2^\frac{h + 3}{2}
                    \end{split}
                \end{equation*}
                Hence, the area is
                \begin{equation*}
                    \begin{split}
                        A_h &= l_h \cdot w_h \\
                            &= 2^\frac{h + 1}{2} \cdot 2^\frac{h + 3}{2} \\
                            &= 2^\frac{2h + 4}{2} \\
                            &= 2^{h + 2} \quad \quad (h = \log (n + 1) - 1) \\
                            &= 2^{\log(n + 1) -1 +2} \\
                            &= 2^{\log(n + 1) +1} \\
                            &= 2(n + 1)
                    \end{split}
                \end{equation*}
            \item For the \tit{even} case, the sides of the rectangle are defined as follows: $$\soe{l}{l_h = 2l_{h - 1} = 2l_{h - 2} \\ w_h = w_{h - 1} = 2w_{h - 2}}$$ (note that $l_{h - 1} = l_{h - 2}$ and $w_{h - 1} = 2w_{h - 2}$) Therefore, the calculations are analogous, but $h - 2k = 0 \implies k = \dfrac{h}{2}$ which leads to $$l_h = w_h = 2^\frac{h + 2}{2}$$ (recall that $l_0 = w_0 = 2$), hence
                \begin{equation*}
                    \begin{split}
                        A_h &= l_h \cdot w_h \\
                            &= 2^\frac{h + 2}{2} \cdot 2^\frac{h + 2}{2} \\
                            &= 2^\frac{2h + 4}{2} \\
                            &= 2^{h + 2} \\
                            &= \ldots \\
                            &= 2(n + 1)
                    \end{split}
                \end{equation*}
        \end{itemize}
    }

    \subsection{The collinear layout}

    The Thompson's Model imposes the restriction that each \tit{processing element} (i.e. node) can have at most \tbf{4 wires} coming out of it in 2D (and 6 in 3D). This constraint ensures that nodes have \textit{manageable connectivity}, which is crucial for simplifying VLSI layouts.

    However, when nodes with \tbf{higher degrees} are \textit{required}, especially in more complex designs, this limitation becomes problematic. By the late 1990s, researchers proposed the following \tbf{non-constant node degree model} as a solution:

    \begin{itemize}
        \item a node with degree $d$ occupies a square with side length proportional to $\Theta(d)$;
        \item the wires connecting these nodes follow \tbf{horizontal or vertical paths} along \tit{grid lines}, similar to how connections are handled in lower-degree models.
    \end{itemize}

    This adaptation maintains simplicity while accommodating \tit{more complex topologies}. This evolution in layout strategies allows for more scalable VLSI design, making it possible to handle larger, more interconnected networks without overly restrictive node degree constraints.

    In particular, this section will focus on a layout proposed by \textcite{yeh}, called the \tbf{collinear layout}, in which all the nodes of the network are placed \tit{on the same line}.

    The following example will show how to get a \tit{collinear layout} from a \tbf{complete graph}.

    \begin{example}[Collinear layouts]
        Consider the following \tit{labeled complete graph}

        \centeredimage[A 6-clique.]{0.2}{../assets/complete_graph.png}

        and let a \tbf{link of type-$i$} be any edge between two nodes whose labels differ by exactly $i$. To obtain the corresponding \tit{collinear layout}, place the 6 nodes on the same line in order, and connect them by placing type-$i$ links in the least possible number of \tbf{tracks} --- in this context a \tit{track} is a horizontal line on which links can be placed. For instance, type-1 links can be placed in 1 track, type-2 links can be placed in 2 tracks --- by placing links between odd numbers on one track and links connecting even numbers on the other --- and so on.

        \centeredimage[Arrangement of links in tracks]{0.2}{../assets/collinear_clique.png}
    \end{example}

    This example shows that type-$i$ links of a collinear layout occupy at most $\min(i, n - i)$. Thus, the total number of tracks of this layout can be obtained by evaluating the following sum:
    
    \begin{equation*}
        \begin{split}
            \sum_{i = 1}^{n - 1}{\min(i, n - i)} &= \sum_{i = 1}^{\frac{n}{2}}{i} + \sum_{i = \frac{n}{2} + 1}^{n - 1}{(n - i)} \\
                                                 &= \sum_{i = 1}^\frac{n}{2}{i} + \sum_{j = 1}^{\frac{n}{2} - 1}{j} \quad \quad (j = n - i) \\
                                                 &= \dfrac{1}{2}\sbk{\dfrac{n}{2} \rbk{\dfrac{n}{2} + 1} + \dfrac{n}{2}\rbk{\dfrac{n}{2} - 1}} \\
                                                 &= \dfrac{n^2}{4}
        \end{split}
    \end{equation*}

    placeholder \todo{incomplete}

    \begin{framedthm}{Thompson's theorem}
        Given a VLSI design, and its corresponding graph $G$, the area occupied by the wires and the nodes of the design is at least $\frac{w^2}{4}$, where $w$ is $G$'s bisection width.
    \end{framedthm}

    \begin{proof}
        The bound of the area of the VLSI design will be proved by counting the minimum number of occupied squares of the grid.

        Consider the VLSI design on a grid as a Cartesian plane, and vertical lines of the form $x = a$. Each possible $x = a$ split the vertices of the network into three separate subsets:

        \begin{itemize}
            \item $L$, which contains the vertices on the left of the vertical line
            \item $R$, which contains the vertices on the right of the vertical line
            \item $S$, which contains the vertices that lie right on the vertical line
        \end{itemize}

        In particular, by monotonicity, there exists an $a$ such that $\abs L + \abs S \ge \dfrac{n}{2}$ and $\abs R + \abs S \ge \dfrac{n}{2}$. Clearly, if $\abs S = 0$, the line $x = a$ cuts the graph into two sets of vertices $L$ and $R$, which must cut at least $w$ edges (by definition of \tit{bisection width}). Otherwise, if $\abs S \neq 0$, $S$ can be split into two subsets $S_1$ and $S_2$, such that $\abs {L \cup S_1} = \abs{R \cup S_2} = \frac{n}{2}$, and the vertical line will still cut at least $w$ edges.

        The line $x = a$ is said to account for the $w$ square units of area of wire and vertices that lie within $\frac{1}{2}$ unit distance of it.

        Consider a \curlyquotes{zig-zag} defined as follows: $$Z_1(x) := \soe{ll}{a - 1 & y \ge b_1 \\ a - 1 \le x \le a + 1 & y = b_1 \\ a + 1 & y \le b_1}$$ where $b_1$ is such that $Z_1$ still bisects the graph, therefore it cuts at least $w$ edges. Note that the horizontal segment of $Z_1$ may cut at most 2 wires, therefore its vertical sections will cross at least $w - 2$ wires.

        Consider each possible zig-zag $$Z_k(x) := \soe{ll}{a - k & y \ge b_k \\ a_ k \le x \le a + k & y = b_k \\ a + k & y \le b_k}$$ where $b_k$ is such that $Z_k$ still bisects the graph, therefore it cuts at least $w$ edges. Since the horizontal segment will cut $2k$ edges, the vertical sections of $Z_k$ will cut $w - 2k$ edges.

        Finally, since $\floor{\dfrac{w}{2}}$ zig-zags can be drawn on the graph (by definition of \tit{bisection width}), the total area of wire and vertices of the VLSI design is at least $$\sum_{k = 0}^{\floor{\frac{w}{2}}} > \dfrac{w^2}{4}$$
    \end{proof}

    \subsection{The Wise layout} \label{wise}

    In a paper published by \textcite{butterwise}, it was proposed a \tit{different layout} for the butterfly network (discussed in \cref{butterfly_networks}), which is shown in the following figure.

    \centeredimage[The alternative layout of the butterfly network.]{0.3}{../assets/butterwise.png}

    Note that inputs are placed in the upper layer, and outputs in the lower layer; also, the \tit{blue circles} represent nodes of the butterfly network, and \tit{black squares} represent \tbf{devices} that allows to avoid interference, since those wire conjunctions are \tit{knock-knees}.

    \centeredimage[Rearrangement to get the Wise layout.]{0.2}{../assets/butterwise_perm.png}

    This rearrangement of the wires allows to have some important \tit{properties}.
    
    \begin{itemize}
        \item All the wires in the same layer have \tbf{equal length}. Note that this is \tit{not true} for every layout; for instance, in the \tit{classical drawing} of the butterfly network, the \tit{straight edges} in the last layer have \tbf{unit length}, while the \tit{cross-edges} in the same layer have lengths that \tbf{scale} linearly with the input size $N$. This disparity is problematic, as it causes a \tit{loss of synchronization} in the information flow. Nevertheless, this length grows exponentially.
        \item The length of the \tbf{longest path} from any input to any output is linear in $N$, namely $2(N - 1)$, and it can be computed by evaluating the diagonal of the square having side length equal to $\sqrt 2 (N - 1)$, shown in figure \todo{add pic}
            which is$$\sqrt{2 \cdot \rbk{\sqrt{2}(N - 1)}^2} = \sqrt{2 \cdot 2 \rbk{N - 1}^2} = \sqrt{4 \cdot \rbk{N - 1}^2} = 2( N - 1)$$
        \item placeholder \todo{pag 20 slide?}
        \item The value of the \tbf{area} of this layout is good, which is $$\rbk{\sqrt 2(N - 1)}^2 = 2N^2 + o(N^2)$$
    \end{itemize}

    Later studies found that the \tbf{area} of this layout is \tit{inaccurate} because the \tbf{slanted} lines --- rotated by 45° --- that define this layout cannot be produced by machines of the fabrication process. In fact, machines can only create \tit{horizontal and vertical lines}, which means that the actual layout on a board would occupy significantly more area.

    \centeredimage[The \tit{actual} Wise butterfly layout.]{0.2}{../assets/rotated_butterwise.png}

    The area of this layout can be evaluated by calculating the area of this \tit{bigger} square, which has side length $2(N - 1)$: $$\rbk{2(N -1)}^2 = 4N^2 + o(N^2)$$ Additionally, \tit{knock-knees} are not avoided, but arranged in the layout thanks to devices that enlarge the layout area even more.

    \subsection{Layered layout}

    In 2000 \textcite{seven} presented a new layout, based on the \tbf{layered cross product} between graphs, which is described below.

    \begin{frameddefn}{Layered graph}
        A \tbf{layered graph} of $l + 1$ layers $G = (V_0, \ldots, V_l, E)$ consists of $l + 1$ layers of nodes, where $V_i$ is the $i$-th node layer, and each edge $(u, v) \in E$ connects $u$ and $v$ if and only if $u \in V_i$ and $v \in V_{i + 1}$ --- i.e. they belong to adjacent layers.
    \end{frameddefn}

    \begin{example}[Layered graphs]
        The following is an example of a layered graph.

        placeholder \todo{add pic}
    \end{example}

    \begin{frameddefn}{Layered cross product}
        Given two layered graphs $G_1 = (V_0^1, \ldots, V_l^1, E^1)$ and $G_2 = (V_0^2, \ldots, V_l^2, E^2)$ of $l + 1$ layers, the \tbf{layered cross product} (LCP) is a new \tit{layered graph} $$G = G_1 \times G_2 := (V_0, \ldots, V_l, E)$$ defined as follows:

        \begin{itemize}
            \item for each $i \in [0, l]$, $V_i := V_i ^1 \times V_i^2$, i.e. each layer in $G$ is the \tit{cartesian product} of the corresponding two layers in $G_1$ and $G_2$;
            \item $((u^1, u^2), (v^1, v^2)) \in E \iff (u^1, u^2) \in E^1 \land (v^1, v^2) \in E_2$, i.e. there is an edge between two nodes of $G$ if and only if the corresponding nodes were connected in the original graphs.
        \end{itemize}
    \end{frameddefn}

    Note that the LCP is not commutative.

    \begin{example}[LCPs]
        The following is an example of an LCP between two graphs.
        
        \centeredimage[An LCP between two graphs.]{0.2}{../assets/lcp.png}
    \end{example}

    LCPs are particularly useful because \textcite{evenlitman} showed that \tit{various topologies} can be defined as LCPs of \tit{simpler structures}, such as trees. Specifically, it can be shown that butterfly networks are LCPs of \tbf{two complete binary trees}, one oriented upward and the other downward.

    \centeredimage[The LCP that defines the 2-dimensional butterfly network.]{0.27}{../assets/butterfly_lcp.png}

    Interestingly, the LCP of two graphs can be evaluated through a method known as \tbf{Projection Methodology} (PM), as illustrated below.

    \centeredimage[The LCP of the two graphs is obtained by this projection.]{0.27}{../assets/lcp_pm.png}

    It is important to note that the PM may produce results that \tit{do not align} with the requirements of the Thompson's Model. For instance, while the projection above still represents the same butterfly network as before, it is not an \tbf{orthogonal drawing}.

    Consider the following projection plane.

    \centeredimage[Possible edge cross products.]{0.2}{../assets/lcp_edges.png}
    
    From these projections, it is evident that:

    \begin{itemize}
        \item the product of \tit{two diagonal edges} yields a \tit{diagonal edge}, which is \tit{not allowed};
        \item the product of a \tit{vertical edge} and a \tit{diagonal edge} yields a \tit{vertical edge}, which is allowed;
        \item the product of a \tit{diagonal edge} and a \tit{vertical edge} yields a \tit{horizontal edge}, which is allowed;
        \item the product of \tit{vertical edges} yields \tit{two overlapping points}, which is \tit{not allowed}.
    \end{itemize}

    Therefore, to achieve a \tit{valid layout} using the PM, it is essential to ensure that the product of \tit{two diagonal edges} or \tit{two vertical edges} \tbf{never occurs}.

    Note that this is not the only problem that may occur in layouts generated through the PM.

    \begin{frameddefn}{Consistent edges}
        Two edges $e_1$ and $e_2$ are said to be \tbf{consistent} if the open intervals of their projections along the same axis are \tit{disjoint}.
    \end{frameddefn}

    \begin{example}[Consistent edges]
        Consider the following edges and their projections on the $x$-axis:

        \centeredimage[Consistent and inconsistent pair of edges, from left to right.]{0.3}{../assets/intervals.png}

        The first two edges are \tit{consistent}, while the other two are not.
    \end{example}

    \tit{Consistency} of edges in the input graphs must be checked, to avoid overlapping wires in the resulting graph. In particular, \tit{two cases} must be avoided.

        In this first scenario, in $G_1$ there are two inconsistent edges in the same layer $i$, and there is an edge in $G_2$ in layer $i$ as well. This produces two overlapping edges in the projection.

        \centeredimage[First inconsistency case]{0.25}{../assets/consistency1.png}

        Note that this situation arises only when the edge in $G_2$ is parallel to the $x$-axis, as it cannot be drawn diagonally, since the inconsistent edges in $G_1$ are already diagonal, and ---  as previously discussed --- the cross product between two diagonal edges must be avoided.

        The second scenario occurs when in $G_1$ there are two inconsistent edges in different layers $i_1$ and $i_2$, and there are collinear edges in $G_2$ in layers $i_1$ and $i_2$ as well. This produces two overlapping edges in the projection.

        \centeredimage[Second inconsistency case]{0.3}{../assets/consistency2.png}

    All the required constraints are summarized in the next proposition.

    \begin{framedprop}{Valid PM layouts}
        Given two graphs $G_1$ and $G_2$, the PM between them generates \tbf{valid layouts}, i.e. in the resulting graph

        \begin{enumerate}
            \item every edge lies on grid lines
            \item at most one node is mapped to each grid point
            \item no pair of edges overlap
        \end{enumerate}
        
        if and only if

        \begin{enumerate}
            \item the cross product of any pair of edges $e_1 \in E_1$ and $e_2 \in E_2$ is such that exactly one between $e_1$ and $e_2$ is drawn diagonally
            \item for each $i \in [0, l]$, it holds that $$\bigcap_{i = 0}^l{\{\abk{u_x,v_z} \mid u \in V_i^1, v \in V_i^2 \}} = \varnothing$$ where $\abk{u_x, v_z}$ is the node at the intersection of the projection lines of $u$ and $v$ along the $x$ ans $z$ axes, respectively
            \item there are no edges in $G_2$ on layers that contain inconsistent edges in $G_1$, and no collinear edges in different layers of $G_2$ in which $G_1$ has inconsistent edges
        \end{enumerate}

        respectively.

    \end{framedprop}

    For the first constraint, a solution is to \tbf{double} the number of layers, such that the edges in the drawing of $G_1$ are diagonal in \tit{odd} layers, and straight in the \tit{even} layers, while the edges in the drawing of $G_2$ are straight in the \tit{odd} layers, and diagonal in the \tit{even} layers.

    \centeredimage[A complete layered binary tree with the nodes doubled.]{0.3}{../assets/doubled.png}

    The second constraint can be addressed by ensuring that no pair of nodes in the drawing of the first (or second) graph, except for the two endpoints of the same straight edge, share the same $x$-coordinate (or $z$-coordinate). This is always achievable by appropriately enlarging the drawings of the two graphs.
    
    Lastly, the third constraint is more difficult to enforce and presents a significant limitation of this technique. For this reason, the focus of this work is restricted to networks where each LCP is calculated from two complete layered binary trees, with double the number of nodes.

    \centeredimage[The LCP of two such trees.]{0.3}{../assets/square_butterfly.png}

    This figure represents the \tbf{planar layout} of a butterfly network, which adheres to all the previously outlined constraints. Moreover

    \begin{itemize}
        \item it is symmetric;
        \item it is a square with side length $2(N - 1)$, therefore the area is $4N^2 + o(N^2)$ --- note that this area is worse than the Wise layout (discussed in \cref{wise}) because in this case the whole area is filled, whereas the Wise layout only uses a portion of such a big area;
        \item all the edges on the same layer have the same length;
        \item unfortunately, input and output nodes do not lie on the boundaries.
    \end{itemize}

    \subsection{Optimal area of the butterfly network}

    \begin{framedlem}{}
        Given an $n$-dimensional butterfly network, for any non-negative integers $j, k > 0$ such that $0 \le j \le j + k \le n$, the subgraph of the butterfly network induces by the nodes in levels $j, j + 1, \ldots, j + k$ is the disjoint union of $2^{n - k}$ copies of $k$-dimensional butterfly networks.
    \end{framedlem}

    In particular, if $j = 0$ and $k = n - 1$, we get have the following \todo{add pic}

    Therefore, an $(n - 1)$-dimensional butterfly network can be built from a pair of $(n - 2)$-dimensional butterfly networks connected by one node layer and one ege layer. \todo{what?}

    \chapter{The worm propagation prevention problem}

    A \href{https://en.wikipedia.org/wiki/Computer_worm}{computer worm} is a type of malware designed to self-replicate and spread across networks without needing a host program. Unlike \tit{viruses}, which require human action to propagate, \tbf{worms} use computer networks to exploit security vulnerabilities in target systems, infiltrating and duplicating themselves automatically.

    Once a worm gains access, it can spread rapidly to other devices, causing network slowdowns, data breaches, or system damage, depending on the worm's purpose. Additionally, worms can carry payloads that steal sensitive data, install other forms of malware, or create backdoors for unauthorized access. Effective network security measures, such as patching vulnerabilities and monitoring traffic, are essential to prevent worm attacks.

    The harmful effects of a worm can be broadly classified into \tit{two categories}:

    \begin{itemize}
        \item \tbf{Direct damage}. These are caused by the worm's execution on the victim's system. It may lead to system instability, data corruption, file deletion, or even the theft of sensitive information. The worm might consume significant system resources, slowing down performance or rendering the machine unusable. They consist solely of instructions to replicate themselves, and typically do not cause severe direct damage beyond consuming computational resources, which can degrade system performance. However, more advanced \tit{direct damage} worms often disrupt the proper functioning of security software, such as antivirus programs and firewalls, making it harder to detect and remove the malware. This interference can severely hinder the normal operation of the infected machine. In many cases, they also act as \tit{carriers} for the automatic installation of \href{https://en.wikipedia.org/wiki/Backdoor_(computing)}{backdoors} or \href{https://en.wikipedia.org/wiki/Keystroke_logging}{keyloggers}, which can later be exploited by attackers or other forms of malware, further compromising the system's security.
        \item \tbf{Indirect damage}. These arise from the methods the worm uses to spread. For example, worms can generate a large volume of traffic while replicating, which can overwhelm networks, disrupt email systems, and lead to costly downtime or loss of productivity. Additionally, their use of social engineering tactics might result in reputational damage or further security breaches. Their damages result from the widespread infection of many computers across a network, creating cascading effects. These type of worms send numerous email messages during replication, flooding inboxes and contributing to email spam, which wastes valuable bandwidth and user attention. They exploit known vulnerabilities in certain software which can lead to software malfunctions, causing instability in the operating system. This often results in system crashes, forced reboots, or even shutdowns, further disrupting normal operations..
    \end{itemize}

    Assume that the time required to transmit information over any connection in a network is constant and denoted as $T$. If a worm successfully infects a set of nodes $C$, and the worm can spread to all other nodes in the network in a single step, then the entire network will be \tbf{infected} within time $T$. Therefore, we are interested in finding the set of nodes $C$ that can lead to a fully infected network.

    The property that every edge in the network is incident to at least one node in the infected set $C$ ensures that the entire network can be infected after the first propagation step. This condition is \tit{sufficient} (though not \tit{necessary}) to guarantee that the worm will spread to all the nodes in the next step.

    From a network manager's perspective, any filter implemented to protect against first-order worm attacks typically reduces communication efficiency. Therefore, \tbf{minimizing} the number of filters is crucial to strike a balance between security and maintaining communication speed.

    Note that, in reality, the situation is more complicated because large-scale networks tend to have \tit{dynamic connections}, meaning the structure of the network changes over time, which can affect the speed and manner of the worm's propagation.

    \section{The vertex cover problem}

    The problem of finding the set $C$ of vertices discussed earlier, where each edge in the network is incident to at least one vertex in $C$, can be reduced to finding the \tbf{minimal vertex cover} of the network graph.
    
    \begin{frameddefn}{Vertex cover}
        Given a graph $G$, a \tbf{vertex cover} for $G$ is a set of vertices $C \subseteq V(G)$ such that every edge in $G$ is incident to at least one vertex in $C$. Using symbols $$\forall (u, v) \in E(G) \quad u \in C \lor v \in C$$
    \end{frameddefn}

    Note that the minimal vertex cover is not unique. Moreover, for any graph $G$, $V(G)$ is trivially a vertex cover for $G$. To find the minimal vertex cover is not trivial, because there are $\powerset(V(G)) = 2^{V(G)}$ possibile subsets of vertices to check.
    
    \begin{example}[Vertex covers]
        The following are two examples of minimal vertex covers for the given graph: \todo{this is wrong}
        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

                \node[circle, draw]  (1) []{};
                \node[circle, draw, fill=red]  (2) [right of = 1]{};
                \node[circle, draw]  (3) [below right of = 2]{};
                \node[circle, draw]  (4) [below left of = 3]{};
                \node[circle, draw, fill=red]  (5) [left of = 4]{};
                \node[circle, draw]  (6) [above left of = 5]{};

                \node[circle, draw]  (1b) [above right of = 3, xshift=50]{};
                \node[circle, draw]  (2b) [right of = 1b]{};
                \node[circle, draw, fill=red]  (3b) [below right of = 2b]{};
                \node[circle, draw]  (4b) [below left of = 3b]{};
                \node[circle, draw, fill=red]  (5b) [left of = 4b]{};
                \node[circle, draw]  (6b) [above left of = 5b]{};

                \path[every node/.style={font=\sffamily\small}]

                (1) edge (2)
                (2) edge (3)
                (3) edge (4)
                (4) edge (5)
                (5) edge (6)
                (6) edge (1)
                (6) edge (3)
                (5) edge (1)

                (1b) edge (2b)
                (2b) edge (3b)
                (3b) edge (4b)
                (4b) edge (5b)
                (5b) edge (6b)
                (6b) edge (1b)
                (6b) edge (3b)
                (5b) edge (1b)
            ;
            \end{tikzpicture}
            \caption{Two minimal vertex covers for a graph.}
        \end{figure}
    \end{example}

    For the reasons mentioned, every minimal vertex cover serves as an excellent starting point for a worm's propagation within a network. Protecting the computers corresponding to the nodes in the \tbf{minimal vertex cover} of the communication graph is crucial. This strategy ensures that every edge in the graph is monitored, thus preventing a worm from exploiting vulnerabilities in unprotected nodes.

    Moreover, if the graph has multiple minimum vertex covers, it is essential to identify and protect \tit{at least} the computers that lie in the \tbf{intersection} of all these covers. This intersection represents the most secure nodes, as they are critical in preventing the spread of the worm across all potential configurations of the network.

    The following definition provides the \tbf{decisional version} of the minimal vertex cover problem.

    \begin{frameddefn}{Minimal Vertex Cover (VC) problem}
        Given a graph $G$, and an integer $k \ge 0$, is there a vertex cover $C$ for $G$ such that $\abs C \le k$?
    \end{frameddefn}

    The VC problem is one of Karp's 21 \NPComplete problems \cite{karp}, which are a collection of well-known computational problems that were classified as \NPComplete shortly after the introduction of the Cook theorem \cite{cook}.

    The VC problem can be also formulated as a 0-1 \tbf{integer linear program} (ILP):
    \begin{itemize}
        \item for every node $v \in V(G)$, we define the variabile $x_v$
        \item for every edge $(i,j) \in E(G)$ we add the constraint $x_i + x_j \geq 1$. This constraint enforces that at least one between $x_i$ and $x_j$ has to be set to 1
    \end{itemize}

    Therefore, we get the following ILP:
    \[\begin{array}{ccc}
        \qquad\qquad\quad
        & \min \; \sum\limits_{i = 1}^n x_i \\\\
        & x_i + x_j \geq 1 & \forall (i,j) \in E(G) \\
        & x \in \{0,1\}^n
    \end{array}\]

    This formulation is a reduction from VC to ILP, implying that the latter is \NPComplete as well.

    Despite the fact that the VC problem is \NPComplete, it is possible to find an \tit{approximate solution} in polynomial time by leveraging its ILP formulation, by employing an ILP approximated solution.

    \subsection{Approximation algorithms}

    There are multiple algorithms for finding approximated solutions for the VC problem. The first algorithm presented is a naïve solution, based on a \tbf{greedy} approach.

    \begin{framedalgo}[label={alg:first_greedy_avc}]{First greedy AVC}
        Given an undirected graph $G$, the algorithm finds an approximated minimal vertex cover for $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{firstGreedyAVC}{$G$}
                \State $V' := \varnothing$
                \State $E' := E(G)$
                \While{$E' \neq \varnothing$}
                    \State Choose $(i, j) \in E'$
                    \State $V' = V' \cup \{i\}$
                    \For{$(i, k) \in E(G)$} \Comment{any edge having $i$ as an endpoint}
                        \State $E' = E' - \{(i, k)\}$
                    \EndFor
                \EndWhile
                \State \tbf{return} $V'$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        At each iteration of the algorithm, an edge $(i, j ) \in E'$ is chosen randomly, then $i$ is added to the vertex cover set $V'$, and any edge having $i$ as an endpoint is removed from $E'$. This ensures that $V'$ is a vertex cover, though it may not be minimal.
    }

    \cost{
        Each edge has to be either explored or removed by the algorithm, therefore the cost is $O(n + m)$.
    }

    Consider a graph formed by two rows of nodes as follows: the upper row has $r$ nodes, and the lower row has $k > r$ nodes. Each node of the upper row is connected to each node on the lower row. Note that the upper row is a \tit{minimal} vertex cover for the graph.

    Suppose that the algorithm always chooses the nodes from the lower row, then the resulting set of nodes is a vertex cover made of $k$ nodes. Thus, the approximation ratio between this set of nodes and the minimal vertex cover is $$\dfrac{k}{r} = \dfrac{n - r}{r}$$
   
    \begin{framedalgo}{Second greedy AVC}
        Given an undirected graph $G$, the algorithm finds an approximated minimal vertex cover for $G$. \\
        \hrule

        \quad
        \label{alg:second_greedy_avc}
        \begin{algorithmic}[1]
            \Function{secondGreedyAVC}{$G$}
                \State $V' := \varnothing$
                \While{$E(G) \neq \varnothing$}
                    \State $v \in \argmax_{v \in V(G)}{\deg(v)}$
                    \State $V' = V' \cup \{v\}$
                    \For{$(u, v) \in E(G)$} \Comment{any edge having $v$ as an endpoint}
                        \State $G\texttt{.remove\_edge(}u, v\texttt{)}$
                    \EndFor
                \EndWhile
                \State \tbf{return} $V'$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        At each step of the algorithm, the vertex with the highest degree $v$ is choosen from the current set of vertices $V(G)$, then $v$ is added to the vertex cover $V'$, and any edge having $v$ as an endpoint is removed from $G$. This ensures that $V'$ is a vertex cover, though it may not be minimal.
    }

    \cost{
        At each iteration, the cost of finding $v$ is $O(m)$, and because the cost of removing an edge from $G$ is $O(n + m)$, we have that the cost of the algorithm is $O(m (n + m))$.
    }

    Note that algorithm can produce a vertex cover $V'$ whose cardinality is \tit{vary far} from optimum. In fact, consider the following graph: \todo{add pic}

    The upper row of nodes consists of $r$ nodes, and the lower row consists of

    \begin{itemize}
        \item $r$ nodes of degree 1
        \item $\floor{\dfrac{r}{2}}$ nodes of degree 2 $\ldots$
        \item in general, $\floor{\dfrac{r}{i}}$ nodes of degree $i$
    \end{itemize}

    meaning that the total number of nodes is $$n = r + \sum_{i = 1}^r{\floor{\dfrac{r}{i}}} \le r + r \sum_{i = 1} ^r{\dfrac{1}{i}} = \Theta(r \log r)$$ (note that the \href{https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)}{harmonic sum} can be approximated by $\Theta(\log r)$). Although the \tbf{optimal} minimal vertex cover is the \tit{upper row} itself, consisting of $r$ nodes, it may happen that the algorithm chooses the \tit{lower row} as vertex cover, as shown in the following figure \todo{add pic}

    This means that there is an approximation ratio of $$\dfrac{\Theta(r \log r)}{r} = \Theta(\log r)$$

    However, there are better algorithms that can find approximated minimal covers  $V'$ for a given graph $G$ such that $\abs{V'} \le 2 \abs {V^*}$, where $V^*$ is a minimal vertex cover.

    \begin{framedalgo}{2-approximation VC}
        Given an undirected graph $G$, the algorithm finds an approximated minimal vertex cover $V'$ for $G$, such that $\abs{V'} \le 2 \abs {V^*}$, where $V^*$ is a minimal vertex cover. \\
        \hrule

        \quad
        \label{alg:2-approx_vc}
        \begin{algorithmic}[1]
            \Function{2approxVC}{$G$}
                \State $V' := \varnothing$
                \State $E' := E(G)$
                \While{$E' \neq \varnothing$}
                    \State Choose $(i, j) \in E'$
                    \State $V' = V' \cup \{i, j\}$
                    \For{$(i, k) \in E(G)$} \Comment{any edge having $i$ as an endpoint}
                        \State $E' = E' - \{(i, k) \}$
                    \EndFor
                    \For{$(j, h) \in E(G)$} \Comment{any edge having $j$ as an endpoint}
                        \State $E' = E' - \{(j, h) \}$
                    \EndFor
                \EndWhile
                \State \tbf{return} $V'$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        The algorithm computes as the \cref{alg:first_greedy_avc}, but both endpoint of the choosen edge are considered in the removal step.
    }

    \cost{
        The cost of the algorithm is the same of the \cref{alg:first_greedy_avc}, which is $O(m(n +m))$.
    }

    \begin{proof}
        We will prove that any $V'$ returned from the algorithm is such that $\abs{V'} \le 2 \abs {V^*}$ for some minimal vertex cover $V^*$. Note that, by construction, $V'$ is a vertex cover for $G$.

        Let $A$ be the set of the edges \tit{choosen} from $E'$. For each edge $(i, j) \in A$, $i$ and $j$ are added to $V'$ by the algorithm, therefore $$\abs{V'} = 2 \abs A$$

        Moreover, all the edges having either $i$ or $j$ as endpoint are removed from $E'$, thus edges in $A$ cannot be incident, which means that there exists a minimal vertex cover $V^*$ such that $$\abs A \le \abs{V^*}$$

        Finally, we have that $$\abs{V'} = 2 \abs A \le 2 \abs{V^*}$$
    \end{proof}

    \begin{framedalgo}{2-approximation VC (ILP)}
        Given an undirected graph $G$, the algorithm returns a vector that represents an approximated minimal vertex cover $V'$ for $G$, such that $\abs{V'} \le 2 \abs {V^*}$, where $V^*$ is a minimal vertex cover. \\
        \hrule

        \quad
        \label{alg:2-approx_vc_ilp}
        \begin{algorithmic}[1]
            \Function{2approxVCilp}{$G$}
                \State Consider the ILP formulation of the VC problem on $G$
                \State Relax the ILP by replacing the $x \in \{0, 1\}^n$ constraint into $x \in [0, 1]^n \subseteq \R^n$
                \State $x^* := \texttt{polyLPSolver()}$
                \State $y^* \in \{0, 1\}^n$
                \For{$i \in [1, n]$}
                    \If{$x_i \ge \frac{1}{2}$}
                        \State $y_i^* := 1$
                    \Else
                        \State $y_i^* := 0$
                    \EndIf
                \EndFor
                \State \tbf{return} $y^*$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        By relaxing the ILP formulation of the VC problem on $G$ to a LP problem, we can use any polynomial LP solver to get a fractional solution $x^* \in [0, 1]^n$. Thus, to get a valid vertex cover, it is sufficient to consider only the $x^*_i$'s such that $x^*_i \ge \frac{1}{2}$.
    }

    \begin{proof}
        Let $V'$ be the set of vertices described by $y^*$. It is easy to see that $V'$ is a vertex cover for $G$, because the constraint of the ILP $$x^*_i + x^*_j \ge 1$$ forces at least one between $x^*_i$ and $x^*_j$ to be greater or equal than $\frac{1}{2}$, therefore at least one between $y^*_i$ and $y_j^*$ will be set to 1.

        Now we will prove that the algorithm returns a 2-approximation of an optimal solution $V^*$. Let $$Z := \sum_{i = 1}^n{x^*_i}$$ Since $x^*_i \le 1$ for each $i \in [1, n]$, it must be that $Z \le \abs{V^*}$ \todo{what?}. Let $y^* \in \{0, 1\}^n$ be the integer solution obtained from $x^* \in [0, 1]^n$ by the rounding procedure; clearly, for each $i \in [1, n]$, we have that $y^*_i \le 2x^*_i$, therefore $$\abs{V'} = y^*_1 + \ldots + y^*_n \le 2 (x^*_1 + \ldots + x^*_n) = 2Z \le 2 \abs{V^*}$$
    \end{proof}

    The vertex cover problem is related to many other graph theory problems.

    \begin{frameddefn}{Independent set}
        Given an undirected graph $G$, $S \subseteq V(G)$ is an \tbf{independent set} if and only if $$\forall v, v' \in S \quad \nexists (v, v') \in E(G)$$
    \end{frameddefn}

    \begin{framedthm}{}
        A set of nodes $V'$ is a vertex cover over a graph $G$ if and only if its complement $V(G) - V'$ is an independent set.
    \end{framedthm}

    \proofiff{
        By way of contradiction, assume that there exist $x, y \in V(G) - V'$ such that $(x, y) \in E(G)$; note that neither $x$ nor $y$ are in $V'$ because they are in its complement, therefore $(x, y)$ is not covered by the vertex cover $V'$.
    }{
        Analogously, by way of contradiction, assume that there exists an edge $(x, y) \in E(G)$ that is not covered by any node in $V'$, then both $x, y \in V(G) - V'$, therefore there exist two adjacent nodes in $V(G) - V'$.
    }

    \begin{framedcor}{}
        The number of nodes of a graph is equal to the size of its minimal vertex cover, plus the size of a maximal independent set.
    \end{framedcor}

    \begin{frameddefn}{Matching}
        Given a graph $G$, $M \subseteq E(G)$ is a \tbf{matching} of $G$ if and only if $$\forall (x, y), (u, v) \in M \quad x , y\neq u,v$$
    \end{frameddefn}

    \begin{frameddefn}{Perfect matching}
        Given a graph $G$, a \tbf{perfect matching} is a matching that covers every vertex of the $G$.
    \end{frameddefn}

    Note that every perfect matching is a \tit{maximal matching}.

    \begin{framedthm}[label={bounded_matching}]{}
        Let $M$ be a matching of $G$ and $C$ a vertex cover for $G$; then $\abs M \le \abs C$.
    \end{framedthm}

    \begin{proof}
        $C$ is a vertex cover, thus it must cover all edges in $E(G)$, and in particular it covers all edges in $M$. By definition of vertex cover, for each edge in $M$, at least one of its endpoints must be in $C$, therefore $\abs C$ must be at least $\abs M$.
    \end{proof}

    \begin{framedcor}{}
        Let $M$ be a matching of $G$ and $C$ a vertex cover for $G$. If $\abs M = \abs C$ then $M$ is a maximal matching and $C$ is a minimal vertex cover.
    \end{framedcor}

    Note that, although a maximal matching can be found in polynomial time, the contrapositive of this corollary is not true in general, therefore it is not possible to try to solve the minimal vertex cover through the maximal matching problem.

    \begin{framedalgo}{2-approximation VC (matching)}
        Given an undirected graph $G$, the algorithm returns a minimal vertex cover $V'$ for $G$, such that $\abs{V'} \le 2 \abs {V^*}$, where $V^*$ is a minimal vertex cover. \\
        \hrule

        \quad
        \label{alg:2-approx_vc_matching}
        \begin{algorithmic}[1]
            \Function{2approxVCmatching}{$G$}
                \State $M := \texttt{findMaximalMatching(}G\texttt{)}$
                \State $V' := \varnothing$
                \For{$(u, v) \in M$}
                    \State $V' = V' \cup \{u, v\}$
                \EndFor
                \State \tbf{return} $V'$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        By computing a maximal matching $M$ on $G$, all the endpoints of the edges in $M$ will form a vertex cover for $G$, by definition of matching.
    }

    \cost{
        Note that the time complexity of the algorithm depends directly on the algorithm used to compute the maximal matching $M$ of $G$.
    }

    \begin{proof}
        Consider an edge $(u, v) \in E(G)$; this edge is either in $M$, and therefore both endpoints are inserted into $V'$ by the algorithm, or $(u, v) \in E(G) - M$, but at least one of its endpoints must be in $V'$, otherwise it could have been put into $M$, but this is not possible because $M$ is maximal. This proves that $V'$ is a vertex cover.

        Let $V^*$ be a minimal vertex cover for $G$; to prove that $V'$ is a 2-approximation of the VC problem, note that by \cref{bounded_matching} we have that $\abs M \le \abs{V^*}$, therefore $$\abs{V'} = 2 \abs M \le 2 \abs{V^*}$$
    \end{proof}

    \begin{frameddefn}{Bipartite graph}
        A graph $G$ is said to be \tbf{bipartite} if and only if there is a partition $U, W$ of $V(G)$ such that both $U$ and $V$ are independent sets.
    \end{frameddefn}

    Bipartite graphs are very important, because the following theorem, proved by König \cite{konig}, allows to compute a maximal VC on bipartite graphs in polynomial time.

    \begin{framedthm}{König's theorem}
        In any bipartite graph, the number of edges in a maximal matching is equal to the number of vertices in a minimal vertex cover.
    \end{framedthm}

    \subsection{The eternal vertex cover problem}

    In \tbf{Dynamic network security}, the \tit{fault-tolerance model}'s objective is to deploy a \tit{minimum} set of guards across network nodes to provide continuous protection against attacks or faults on any single network link at any time. When an attack or fault occurs on a link, a guard stationed at one of the adjacent nodes detects it and \tit{immediately} moves across the link to defend or repair the issue. Meanwhile, the remaining guards reconfigure by repositioning themselves to adjacent nodes. This reconfiguration ensures that the system remains \tbf{protected} from future attacks or failures, maintaining a dynamic, adaptive defense mechanism.

    This process ensures that protection is not only \tit{instantaneous} but can be maintained \tit{indefinitely}. The guards adjust their positions in response to each new incident, guaranteeing continuous defense against single-link attacks or failures in an \tit{ad infinitum} manner, adapting dynamically to evolving threats or faults without compromising the network's resilience.

    This model can be translated into the \tbf{eternal vertex cover} problem, in which

    \begin{itemize}
        \item the network is modeled as a graph
        \item at most one defender is located at each node
        \item an attacker can target edges
        \item a defender \tit{can} protect all the edges incident to the nodes where the guard is located
        \item to defend an attacked edge, a guard must move along the attacked edge
        \item any guard can traverse one edge at a time
    \end{itemize}

    Given the set of nodes where guards are deployed at any moment, if these nodes do not form a \tbf{vertex cover}, the attacker can exploit any uncovered edge to bypass the defense and successfully breach the network. Therefore, to ensure continuous protection, the defender must \tit{dynamically reconfigure} the guard positions so that the current set of guarded nodes always forms a valid vertex cover. This reconfiguration is crucial after any attack, transforming \tit{one vertex cover into another} in response to the attack, ensuring that no edge remains exposed.

    If $\alpha (G)$ is the cardinality of a minimal VC on a graph $G$, and $\alpha ^{\infty}(G)$ is the cardinality of a minimal eternal VC, then $$\alpha(G) \le \alpha^{\infty}(G)$$

    \begin{framedthm}{Shadow guard}
        Let $G$ be a connected graph, and let $V'$ be a vertex cover for $G$ that induces a connected subgraph of $G$. Then $\alpha^{\infty}(G) \le \abs{V'} + 1$.
    \end{framedthm}

    \begin{proof}
        Choose a vertex $d \in V(G) - V'$, and place a new guard on it; we will refer to this guard as \tit{shadow guard}. Let $P$ be the following path $$P = d, v_1, \ldots, v_k, x$$ where $v_1, \ldots, v_k \in V'$, and $(v_k, x)$ is the attacked edge. To defend $(v_k, x)$, it is sufficient to slide each guard over $P$, towards $x$, therefore the \tit{shadow guard} will now be on $v_1$, and the attacked edge will be defended by the guard that slid from $v_k$ to $x$.

        Finally, note that the new set of vertices on which the guards now stand on still form a vertex cover, hence $V' \cup \{d\}$ is an eternal vertex cover.
    \end{proof}

    \begin{framedlem}{}
        Let $G$ be a connected graph, and let $V'$ be a vertex cover inducing a subgraph of $G$, with $k$ connected components. Then $\alpha^{\infty}(G) \le \abs{V'} + k$.
    \end{framedlem}

    \begin{proof}
        Considering each connected component of $V'$ as a separate connected subgraph, we get the result of the lemma, by the same reasoning of the previous theorem.
    \end{proof}

    Note that $V'$ can induce at most $\abs{V'}$ connected components, therefore we get the following theorem.

    \begin{framedthm}{}
        Given a graph $G$, we have that $$\alpha(G) \le \alpha^{\infty} (G) \le 2 \alpha(G)$$
    \end{framedthm}

    \begin{framedthm}{Eternal VC on cycles}
        For any $n \ge 3$, we have that $$\alpha^{\infty}(C_n) = \alpha(C_n) = \ceil{\frac{n}{2}}$$ where $C_n$ is a cycle graph of $n$ nodes.
    \end{framedthm}

    \begin{proof}
        By placing a guard on alternated nodes of the cycle graph $C_n$, we get an eternal vertex cover: if an edge is attacked, it is sufficient to rotate each guard by 1.
    \end{proof}

    \begin{framedthm}{Eternal VC on paths}
        For any $n \ge 1$, we have that $$\alpha^{\infty}(P_n) = n - 1$$ where $P_n$ is a path graph of $n$ nodes.
    \end{framedthm}
    
    \begin{proof}
        Consider a vertex cover of less than $n - 1$ nodes of $P_n$. It is always possible to design an attack strategy such that all the guards form a connected path, therefore an edge will be unprotected because there are less than $n - 1$ guards.
    \end{proof}

    \chapter{TODO}

    TODO

    \section{The Traveling Salesman Problem}

    \begin{frameddefn}{Traveling Salesman Problem}
        The Traveling Salesman Problem (TSP) is defined as follows: a salesman has to visit a given set of cities, such that his tour ends on the same city on which he started, while minimizing the total length of the trip.
    \end{frameddefn}

    The origins of the TSP are somewhat ambiguous:

    \begin{itemize}
        \item an 1832 handbook for traveling salesmen references the problem, presenting example routes through Germany and Switzerland, though without any mathematical formulation
        \item in the mid-1800s, mathematicians \href{https://en.wikipedia.org/wiki/William_Rowan_Hamilton}{W. R. Hamilton} and \href{https://en.wikipedia.org/wiki/Thomas_Kirkman}{T. Kirkman} introduced the first formal mathematical formulation of the problem
        \item the TSP in its general form was studied in the 1930s, when researchers analyzed the limitations of the brute-force algorithm and noted the non-optimality of simpler heuristics like the nearest neighbour approach
    \end{itemize}

    \begin{frameddefn}{Hamiltonian cycle}
        Given a graph $G = (V,E)$, a \tbf{Hamiltonian cycle} (HC) is a cycle that passes through each node in $V(G)$ exactly once.
    \end{frameddefn}

    It can be proven that determining whether a graph $G$ contains a HC is \NPComplete --- HC will be used interchangeably for \curlyquotes{Hamiltonian cycle} and the associated decision problem. Now, consider the following decisional version of the TSP.

    \begin{frameddefn}{TSP (decisional version)}
        Let $K_n = (V, E)$ be a complete graph having $n$ nodes, $\func{w}{E(G)}{\R^+}$ be a non-negative edge-weight function, and $t \ge 0$; does $K_n$ contain a Hamiltonian cycle with total cost at most $t$?
    \end{frameddefn}

    Note that any complete graph $K_n$ trivially contains a HC, but the problem aims at minimizing the cost of the HC. The following proof shows that the TSP is \NPComplete as well.

    \begin{framedthm}[label={tsp np compl}]{$\mathrm{TSP} \in \NPComplete$}
        The TSP is \NPComplete.
    \end{framedthm}

    \begin{proof}
        It can be easily proved that TSP is in \NPclass: given a complete graph $K_n =(V, E)$, and a walk over $K_n$, it can be checked in polynomial time if the walk is a Hamiltonian cycle, and its total weight is bounded by $t$.

        Now it will be proven that TSP is \NPHard, by reducing HC to TSP as follows:

        \begin{itemize}
            \item consider a graph $G = (V, E)$, and construct the complete graph $K_n = (V, E')$, where $E(G) \subseteq E'(K_n)$ and the remaining edges are the ones added to make $K_n$ a complete graph --- note that $n := \abs{V(G)}$
            \item let $t := n$ and define $\func{w}{E'(K_n)}{\R^+}$ as follows: $$\soe{ll}{w(i, j) = 1 & (i, j) \in E(G) \\ w(i, j) = 2 & (i, j) \in E'(K_n) - E(G)}$$
            \item assume that there exists a Hamiltonian cycle $C$ in $G$; by definition of $w$, all edges of $C$ in $K_n$ have weight 1, since they are all in $G$; this shows that if $G$ has a HC, then $K_n$ has a traveling salesman tour of cost $n = t$
            \item conversely, if $K_n$ has a traveling salesman (TS) tour of cost $n$, all edges of the tour necessarily have weight 1, because edges with weight 2 would not minimize the cost; therefore, by definition of $w$, this tour describes a HC in $G$
            \end{itemize}
    \end{proof}

    In 1954 \textcite{dantzig} showed that the TSP can be formulated as an ILP as well:

    \begin{itemize}
        \item given a complete graph $K_n = (V, E)$, and assume that the TS tour is oriented
        \item define variables $x_{ij}$ and $w_{ij}$ for each $(i, j) \in E(G)$
        \item let $x_{ij} = 1$ if and only if the TS tour traverses the oriented edge $(i, j)$
        \item let $w_{ij} = w(i, j)$
    \end{itemize}

    Then, the TSP problem can be formulated as an ILP as follows:

    \[\begin{array}{ccc}
        \qquad\qquad\quad
        & \min \; \sum\limits_{i = 1}^n {\sum\limits_{j = 1}^n {w_{ij}x_{ij}}} \\\\
        & \sum\limits_{i = 1}^n {x_{ij}} = 1, \quad j \in V(K_n) \\
        & \sum\limits_{j = 1}^n {x_{ij}} = 1, \quad i \in V(K_n) \\
        & \sum\limits_{i, j \in S} {x_{ij}} < \abs S, \quad S \subsetneq V(K_n), S \neq \varnothing \\
        & x \in \{0,1\}^n
    \end{array}\]

    The first two constraints force the tour to be Hamiltonian, by imposing that for any vertex $j$ there must be at most 1 incoming edge, and for each vertex $j$ there must be at most 1 outgoing edge. However, this does not imply that the solution of the ILP is a cycle: in fact, without the third constraint, a valid solution could involve multiple unconnected cycles of $G$. Therefore, the last constraint imposes that any proper subsets of vertices $S$ of $V(K_n)$ must cover a number of edges that is strictly less than the number of vertices of $S$ itself; in fact, this constraint avoids the possibility of forming cycles in the solution because a cycle has the same number of edges and vertices.

    The next theorem shows that, in addition to being \NPComplete, the TSP is also non-approximable, making it an especially challenging problem.

    \begin{framedthm}{Inapproximability of the TSP}
        If there exists a polynomial time algorithm for the TSP with any approximation ratio $r > 1$, then $\Pclass = \NPclass$.
    \end{framedthm}

    \begin{proof}
        Let $G = (V, E)$ be an instance of HC, and construct a complete graph $K_{\abs V} = (V, E')$ starting from $G$ by adding edges; moreover, define $\func{w}{E(K_n)}{\R^+}$ as follows: $$\soe{ll}{w(i, j) = 1 & (i, j) \in E(G) \\ w(i, j) = 2 + (r - 1) n & (i,j) \in E'(K_n) - E(G)}$$ Note that, for the same reasoning applied in the proof of \cref{tsp np compl}, a TS tour with total weight $n$ exists in $K_n$ if and only if $G$ has a HC.

        Assume there exists an $r$-approximation polynomial algorithm $A$ for the TSP; therefore, because the total weight of the TS tour is $n$, $A$ ran on $K_n$ would find a solution $H$ such that $$\sum_{(i, j) \in H} {w(i, j)} \le rn$$ Now, assume that there exists an edge $(\hat i, \hat j)$ in $H$ such that $(\hat i, \hat j) \in E'(K_n) - E(G)$; hence, by definition of $w$, the total weight of $H$ must be at least $$\sum_{(i, j) \in H}{w(i, j)} = (n - 1) \cdot 1  + 2 + (r - 1)n = n - 1 + 2 + rn - n = rn + 1 > rn$$ because $H$ would be a cycle containing $n - 1$ edges from $E(G)$ and 1 edge from $E'(K_n) - E(G)$. Since $A$ is an $r$-approximation algorithm, this implies that any solution $H$ for $A$ must contain only edges inside $G$, otherwise $H$ would not be an $r$-approximation of an optimal solution for the TSP. However, if any of $A$'s solutions $H$ lie entirely in $G$, then $H$ is a HC for $G$ as previously discussed, which means that $A$ can find a HC in $G$ in polynomial time, which would imply that $\Pclass = \NPclass$ because HC is \NPComplete.
    \end{proof}

    \subsection{Special cases for the TSP}

    Despite this result showing that the TSP is not generally approximable, it is still possible to find effective approximation algorithms for certain special cases. Note that, for any set of edges $E$, the following notation will be used $$w(E) := \sum_{(i, j) \in E}{w(i,j)}$$
    
    \begin{framedlem}[label={tsp bound}]{Lower bound on TS tours}
        Given a graph $G$ and a weight function $\func{w}{E(G)}{\R^+}$, the weight of any TS tour on $G$ is at least the weight of any MST of $G$.
    \end{framedlem}

    \begin{proof}
        Consider a graph $G$, an MST $T$ of $G$, and an optimal TS tour $H^*$ on $G$. Clearly, if by removing and edge from $H*$, we obtain a path $P$, which has weight strictly less than $H^*$'s weight --- note that this is true because $w(i, j) \in \R^+$ for any $(i, j) \in E(G)$. Moreover, note that a path is a special case of a tree, therefore $P$'s weight must be at least $T$'s weight, by definition of MST. Thus, we have that $$w(T) \le w(P) \le w(H^*)$$
    \end{proof}

    Consider the following special case of graphs.

    \begin{frameddefn}{Metric graphs}
        Given a graph $G$, and a weight function $\func{w}{E(G)}{\R^+}$, $G$ is said to be a \tbf{metric graph} if and only if $$\forall u, v, z \in V(G) \quad w(u, z) \le w(u, v) + w(v, z)$$ which means that $w$ satisfies the triangle inequality.
    \end{frameddefn}

    For metric graphs, there exist algorithm that can approximate solutions for the TSP. In particular, the following algorithm leverages the triangle inequality property of $w$ to obtain a 2-approximation for the TSP.

    \begin{framedalgo}{2-approximation TSP}
        Given a complete graph $K_n = (V, E')$, and a weight function $\func{w}{E'(G)}{\R^+}$ such that $K_n$ is a metric graph, the algorithm finds a TS tour $H$ such that $w(H) \le 2w(H^*)$, where $H^*$ is an optimal TS tour. \\
        \hrule

        \quad
        \label{alg:2-approx_tsp}
        \begin{algorithmic}[1]
            \Function{2approxTSP}{$G$, $w$}
                \State Choose $r \in V(K_n)$ randomly
                \State $T := \texttt{findMST}(K_n, r)$ \Comment{find an MST rooted in $r$}
                \State $\texttt L := \texttt{DFSpreorder}(T)$ \Comment{a \tit{preorder} DFS on $T$}
                \State $V := \varnothing$
                \State $\texttt L' := \texttt{[]}$
                \For{$v \in \texttt L$}
                    \If{$v \notin V$}
                        \State $L'.\texttt{append}(v)$
                        \State $V = V \cup \{v\}$
                    \EndIf
                \EndFor
                \State \textbf{return} \ttt L' \Comment{\texttt L' is \texttt L without repetitions}
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \begin{proof}
        We will prove that any solution $H$ of the algorithm is a 2-approximation of an optimal solution $H^*$ for the TSP.

        Consider an optimal TS tour $H^*$, and an MST $T$ of the complete graph $K_n$ in input, rooted in some $r \in V(K_n)$. The list \texttt L computed by the algorithm is obtained from a \tit{preorder} DFS visit $T$, therefore each \tit{edge} of the visit \texttt L will appear exactly \tit{twice}. This means that the tour $C$ described by the edges between the vertices of \texttt L is such that $w(C) = 2 w(T)$.

        Note that $C$ is not a TS tour, since there nodes are repeated. However, by leveraging the triangle inequality of $w$, we can prove that the weight of the final list \texttt L' --- which is \texttt L without repetitions of the vertices --- is bounded by the weight of \texttt L. In fact, for any instance \texttt L in which $$\ldots \ u \ v \ z \ \ldots$$ where $v$ is repeated, by removing $v$ and passing through $(u, z)$ directly --- which always exists because $K_n$ is a complete graph --- will not worsen the total weight of the tour, because $$w(u, z) \le w(u, v) + w(v, z)$$ by hypothesis. Let $H$ be the tour described by the edges between the vertices of \texttt L'; hence, we have that $w(H) \le w(C)$.

        Finally, because of \cref{tsp bound}, we conclude that $$w(H) \le w(C) = 2w(T) \le 2w(H^*)$$
    \end{proof}

    In 1976 \textcite{christofides} \todo{find citation} showed that it is possible to obtain a better approximation of the TSP problem, because the algorithm previously discussed does not exploit all the available edges on the graph.

    \begin{framedlem}{Handshaking lemma}
        Given a graph $G$, the sum of all the degrees of the vertices in $V(G)$ is $2 \abs E$.
    \end{framedlem}

    \begin{framedcor}[label={odd even}]{}
        The number of vertices that have an odd degree in a graph is even.
    \end{framedcor}

    \begin{proof}
        Consider a graph $G$; for the handshaking lemma, we have that $$\sum_{v \in V(G)}{\deg(v)} = 2 m$$ Let $O: = \{v \in V(G) \mid \deg(v) \ \mathrm{odd}\}$ and $E := \{v \in V(G) \mid \deg(v) \ \mathrm{even}\}$ then, we have that $$\sum_{v \in O}{\deg(v)} + \sum_{v \in E}{\deg(v)} = \sum_{v \in V(G)} {\deg(v)} = 2m$$ because $O$ and $E$ describe a partition on $V(G)$. Therefore, because each degree of nodes in $E$ is even by definition, $\sum_{v \in E}{\deg(v)}$ is even, which means that the handshaking lemma is satisfied only if $\sum_{v \in O}{\deg(v)}$ is even as well. However, since each degree of nodes in $O$ is odd by definition, it must be that the entire sum is even.
    \end{proof}

    \begin{framedalgo}{$\frac{3}{2}$-approximation TSP}
        Given a complete graph $K_n = (V, E')$, and a weight function $\func{w}{E'(G)}{\R^+}$ such that $K_n$ is a metric graph, the algorithm finds a TS tour $H$ such that $w(H) \le \frac{3}{2}w(H^*)$, where $H^*$ is an optimal TS tour. \\
        \hrule

        \quad
        \label{alg:3/2-approx_tsp}
        \begin{algorithmic}[1]
            \Function{3/2approxTSP}{$G$, $w$}
                \State Choose $r \in V(K_n)$ randomly
                \State $T := \texttt{findMST}(K_n, r)$ \Comment{find an MST rooted in $r$}
                \State $O := \{v \in V(T) \mid \deg_T(v) \ \mathrm{odd}\}$ \Comment{$\deg_T(v)$ is the degree of $v$ in $T$}
                \State $M := \texttt{findMinWeightPM}(G^O)$
                \State $U := M \cup T$ \Comment{$U$ is a multi-graph}
                \State $L := \texttt{findEulerianCircuit}(U)$
                \State $V := \varnothing$
                \State $\texttt L' := \texttt{[]}$
                \For{$v \in \texttt L$}
                    \If{$v \notin V$}
                        \State $L'.\texttt{append}(v)$
                        \State $V = V \cup \{v\}$
                    \EndIf
                \EndFor
                \State \textbf{return} \ttt L' \Comment{\texttt L' is \texttt L without repetitions}
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \begin{proof}
        We will prove that any solution $H$ of the algorithm is a $\frac{3}{2}$-approximation of an optimal solution $H^*$ for the TSP.

        Consider an MST $T$ of $K_n$ rooted in some node $r \in V(K_n)$, and consider the set of vertices $O$ that have odd degree in $T$, and the subgraph $G^O$ this set induces. Note that, because of \cref{odd even}, we have that $\abs O$ is even. Now, consider the graph $G^O$, induced by $O$, and a minimum weight perfect matching $M$ of $G^O$. Clearly, the graph induced by the union $M \cup T$ is a multi-graph, and it is connected because $T$ is a spanning tree. 
    \end{proof}

    \printbibliography
\end{document}
